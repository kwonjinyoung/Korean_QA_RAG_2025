import json
import torch
from torch.utils.data import Dataset
import random
import sys
import os
from pathlib import Path

# RAG ì‹œìŠ¤í…œ ì„í¬íŠ¸
sys.path.append(str(Path(__file__).parent.parent / "rag_data_maker"))
try:
    from qdrant_client import QdrantClient
    from maker import BGEM3Embeddings
    RAG_AVAILABLE = True
except ImportError:
    print("âš ï¸ RAG ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. RAG ê¸°ëŠ¥ ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")
    RAG_AVAILABLE = False


class RAGRetriever:
    """RAG ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ê¸°"""
    
    def __init__(self, collection_name="korean_rag_collection", qdrant_url="http://localhost:6333", top_k=3):
        self.collection_name = collection_name
        self.top_k = top_k
        self.client = None
        self.embeddings = None
        self.is_initialized = False
        
        if not RAG_AVAILABLE:
            print("âš ï¸ RAG ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            self.client = QdrantClient(url=qdrant_url, check_compatibility=False)
            
            # ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸
            collections = self.client.get_collections().collections
            collection_names = [col.name for col in collections]
            
            if collection_name not in collection_names:
                print(f"âš ï¸ ì»¬ë ‰ì…˜ '{collection_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return
            
            # BGE-M3 ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
            self.embeddings = BGEM3Embeddings()
            self.is_initialized = True
            print(f"âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ (ì»¬ë ‰ì…˜: {collection_name})")
            
        except Exception as e:
            print(f"âš ï¸ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
    
    def retrieve(self, query: str) -> str:
        """ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
        if not self.is_initialized:
            return ""
        
        try:
            # Dense ê²€ìƒ‰ ìˆ˜í–‰
            query_embedding = self.embeddings.embed_query(query)
            search_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=("dense", query_embedding),
                limit=self.top_k
            )
            
            if not search_results:
                return ""
            
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            context_parts = []
            for i, result in enumerate(search_results, 1):
                text = result.payload.get("text", "").strip()
                score = result.score
                if text and score > 0.3:  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
                    context_parts.append(f"[ì°¸ê³ ìë£Œ {i}] (ìœ ì‚¬ë„: {score:.3f})\n{text}")
            
            return "\n\n".join(context_parts)
            
        except Exception as e:
            print(f"âš ï¸ RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return ""


class CustomDataset(Dataset):
    def __init__(self, fname, tokenizer, max_length=2048, use_rag=True, rag_top_k=3):
        IGNORE_INDEX = -100
        self.inp = []
        self.label = []
        self.max_length = max_length
        self.use_rag = use_rag

        # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.rag_retriever = None
        if use_rag and RAG_AVAILABLE:
            self.rag_retriever = RAGRetriever(top_k=rag_top_k)
            if not self.rag_retriever.is_initialized:
                print("âš ï¸ RAG ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ì¼ë°˜ ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                self.use_rag = False

        PROMPT = '''You are a helpful AI assistant specialized in Korean language, culture, history, grammar, and various academic fields. \
            ë‹¹ì‹ ì€ í•œêµ­ì˜ ì „í†µ ë¬¸í™”ì™€ ì—­ì‚¬, ë¬¸ë²•, ì‚¬íšŒ, ê³¼í•™ê¸°ìˆ  ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì— ëŒ€í•´ ì˜ ì•Œê³  ìˆëŠ” ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. \
            ì œê³µëœ ì°¸ê³ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. \
            ì°¸ê³ ìë£Œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ì—†ë‹¤ë©´ ì¼ë°˜ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ë˜, ì¶”ì¸¡ì´ë‚˜ ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì œê³µí•˜ì§€ ë§ˆì„¸ìš”. \
            ë‹¨, ë™ì¼í•œ ë¬¸ì¥ì„ ì ˆëŒ€ ë°˜ë³µí•˜ì§€ ë§ˆì‹œì˜¤.'''

        with open(fname, "r", encoding="utf-8") as f:
            data = json.load(f)

        def make_chat(inp):
            # question typeë³„ instruction ì •ì˜
            type_instructions = {
                "ì„ ë‹¤í˜•": (
                    "[ì§ˆë¬¸]ì„ ì˜ ì½ê³  ì°¸ê³ ìë£Œë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ì‹œì˜¤. ë¬¸ì œë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì§€ ë§ˆì‹œì˜¤.\n"
                    "[ì§€ì¹¨]\n"
                    "ì£¼ì–´ì§„ ë³´ê¸° ì¤‘ì—ì„œ ê°€ì¥ ì ì ˆí•œ ë‹µì„ ìˆ«ìë¡œë§Œ ì‘ë‹µí•˜ì‹œì˜¤.\n"
                    "ì°¸ê³ ìë£Œê°€ ìˆë‹¤ë©´ ì´ë¥¼ ê·¼ê±°ë¡œ íŒë‹¨í•˜ê³ , ì—†ë‹¤ë©´ ì¼ë°˜ì ì¸ ì§€ì‹ì„ í™œìš©í•˜ì‹œì˜¤.\n\n"
                    "[ì˜ˆì‹œ]\n"
                    "ì§ˆë¬¸: ë‹¤ìŒ í•œêµ­ì˜ ì „í†µ ë†€ì´ ì¤‘ 'ì¡°ì„ ì‹œëŒ€'ì— í–‰í•œ ë†€ì´ëŠ”?\n"
                    "1) ì£¼ì‚¬ìœ„ ë†€ì´\n"
                    "2) ê²€ë¬´\n"
                    "3) ê²©êµ¬\n"
                    "4) ì˜ê³ \n"
                    "5) ë¬´ì• ë¬´\n"
                    "ë‹µë³€: 3"
                ),
                "ì„œìˆ í˜•": (
                    "[ì§ˆë¬¸]ì„ ì˜ ì½ê³  ì°¸ê³ ìë£Œë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ì‹œì˜¤. ë¬¸ì œë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì§€ ë§ˆì‹œì˜¤.\n"
                    "[ì§€ì¹¨]\n"
                    "ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì™„ì„±ëœ ë¬¸ì¥ìœ¼ë¡œ ì„œìˆ í•˜ì‹œì˜¤.\n"
                    "ì°¸ê³ ìë£Œê°€ ìˆë‹¤ë©´ ì´ë¥¼ ê·¼ê±°ë¡œ ì„¤ëª…í•˜ê³ , ì—†ë‹¤ë©´ ì¼ë°˜ì ì¸ ì§€ì‹ì„ í™œìš©í•˜ì‹œì˜¤.\n\n"
                    "[ì˜ˆì‹œ]\n"
                    "ì§ˆë¬¸: ëŒ€í•œë¯¼êµ­ì˜ í–‰ì •êµ¬ì—­ ì²´ê³„ë¥¼ ì„œìˆ í•˜ì„¸ìš”.\n"
                    "ë‹µë³€: ëŒ€í•œë¯¼êµ­ì˜ í–‰ì •êµ¬ì—­ì€ ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ì§€ì—­ ë‹¨ìœ„ë¡œ ë‚˜ë‰˜ì–´ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ë¨¼ì € íŠ¹ë³„ì‹œì™€ ê´‘ì—­ì‹œë¶€í„° ì‚´í´ë³¼ ìˆ˜ ìˆë‹¤. íŠ¹ë³„ì‹œë¡œëŠ” ìˆ˜ë„ì¸ ì„œìš¸íŠ¹ë³„ì‹œê°€ ìˆìœ¼ë©°, ê´‘ì—­ì‹œì—ëŠ” ì¸ì²œê´‘ì—­ì‹œ, ë¶€ì‚°ê´‘ì—­ì‹œ, ëŒ€ì „ê´‘ì—­ì‹œ, ê´‘ì£¼ê´‘ì—­ì‹œ, ëŒ€êµ¬ê´‘ì—­ì‹œ, ìš¸ì‚°ê´‘ì—­ì‹œ ë“±ì´ í¬í•¨ëœë‹¤. ì´ ì™¸ì—ë„ ëŒ€í•œë¯¼êµ­ì€ ì¼ë°˜ ë„ ë‹¨ìœ„ë¡œ 6ê°œì˜ ë„ë¥¼ ë‘ê³  ìˆëŠ”ë°, ê·¸ ì´ë¦„ì€ ê²½ê¸°ë„, ì¶©ì²­ë¶ë„, ì¶©ì²­ë‚¨ë„, ì „ë¼ë‚¨ë„, ê²½ìƒë¶ë„, ê²½ìƒë‚¨ë„ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. íŠ¹ë³„í•œ ìì¹˜ê¶Œì„ ë¶€ì—¬ë°›ì€ ë„ì¸ íŠ¹ë³„ìì¹˜ë„ë¡œëŠ” ì œì£¼íŠ¹ë³„ìì¹˜ë„, ì „ë¶íŠ¹ë³„ìì¹˜ë„, ê°•ì›íŠ¹ë³„ìì¹˜ë„ê°€ ìˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ íŠ¹ë³„ìì¹˜ì‹œë¡œëŠ” ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œê°€ ì¡´ì¬í•œë‹¤."
                ),
                "ë‹¨ë‹µí˜•": (
                    "[ì§ˆë¬¸]ì„ ì˜ ì½ê³  ì°¸ê³ ìë£Œë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ì‹œì˜¤. ë¬¸ì œë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì§€ ë§ˆì‹œì˜¤.\n"
                    "[ì§€ì¹¨]\n"
                    "ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ 2ë‹¨ì–´ ì´ë‚´ë¡œ ê°„ë‹¨íˆ ë‹µí•˜ì‹œì˜¤.\n"
                    "ì°¸ê³ ìë£Œê°€ ìˆë‹¤ë©´ ì´ë¥¼ ê·¼ê±°ë¡œ íŒë‹¨í•˜ê³ , ì—†ë‹¤ë©´ ì¼ë°˜ì ì¸ ì§€ì‹ì„ í™œìš©í•˜ì‹œì˜¤.\n\n"
                    "[ì˜ˆì‹œ]\n"
                    "ì§ˆë¬¸: ì¡°ì„  í›„ê¸°ì˜ ì‹¤í•™ ì‚¬ìƒê°€ë¡œ ëª©ë¯¼ì‹¬ì„œë¥¼ ì“´ ì¸ë¬¼ì€?\n"
                    "ë‹µë³€: ì •ì•½ìš©"
                ),
                "êµì •í˜•": (
                    "[ì§ˆë¬¸]ì„ ì˜ ì½ê³  ì°¸ê³ ìë£Œë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ì‹œì˜¤. ë¬¸ì œë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì§€ ë§ˆì‹œì˜¤.\n"
                    "[ì§€ì¹¨]\n"
                    "ì£¼ì–´ì§„ ë¬¸ì¥ì´ ì˜¬ë°”ë¥¸ì§€ íŒë‹¨í•˜ê³ , í‹€ë¦° ê²½ìš° ì˜¬ë°”ë¥´ê²Œ êµì •í•˜ì—¬ \"~ê°€ ì˜³ë‹¤.\" í˜•íƒœë¡œ ë‹µë³€í•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì‹œì˜¤.\n"
                    "ì°¸ê³ ìë£Œì˜ ë¬¸ë²• ê·œì •ì„ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì‹œì˜¤.\n\n"
                    "[ì˜ˆì‹œ]\n"
                    "ì§ˆë¬¸: ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ì–´ë¬¸ ê·œë²”ì— ë¶€í•©í•˜ì§€ ì•ŠëŠ” ë¶€ë¶„ì„ ì°¾ì•„ ê³ ì¹˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\n\"ì˜¤ëŠ˜ì€ í¼ì¦ ë§ˆì¶”ê¸°ë¥¼ í•´ ë³¼ ê±°ì˜ˆìš”.\"\n"
                    "ë‹µë³€: \"ì˜¤ëŠ˜ì€ í¼ì¦ ë§ì¶”ê¸°ë¥¼ í•´ ë³¼ ê±°ì˜ˆìš”.\"ê°€ ì˜³ë‹¤. 'ì œìë¦¬ì— ë§ê²Œ ë¶™ì´ë‹¤, ì£¼ë¬¸í•˜ë‹¤, ë˜‘ë°”ë¥´ê²Œ í•˜ë‹¤, ë¹„êµí•˜ë‹¤' ë“±ì˜ ëœ»ì´ ìˆëŠ” ë§ì€ 'ë§ˆì¶”ë‹¤'ê°€ ì•„ë‹Œ 'ë§ì¶”ë‹¤'ë¡œ ì ëŠ”ë‹¤."
                ),
                "ì„ íƒí˜•": (
                    "[ì§ˆë¬¸]ì„ ì˜ ì½ê³  ì°¸ê³ ìë£Œë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ì‹œì˜¤. ë¬¸ì œë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì§€ ë§ˆì‹œì˜¤.\n"
                    "[ì§€ì¹¨]\n"
                    "ì£¼ì–´ì§„ ë³´ê¸°ë“¤ ì¤‘ì—ì„œ ê°€ì¥ ì ì ˆí•œ ê²ƒì„ ì„ íƒí•˜ì—¬ \"~ê°€ ì˜³ë‹¤.\" í˜•íƒœë¡œ ë‹µë³€í•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì‹œì˜¤.\n"
                    "ì°¸ê³ ìë£Œì˜ ë¬¸ë²• ê·œì •ì„ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì‹œì˜¤.\n\n"
                    "[ì˜ˆì‹œ]\n"
                    "ì§ˆë¬¸: \"ë‚˜ëŠ” ê·¸ë¥¼ ë³¸ ì ì´ ìˆìŒì„ {ê¸°ì–µí•´ëƒˆë‹¤/ê¸°ì–µí•´ ëƒˆë‹¤}.\" ê°€ìš´ë° ì˜¬ë°”ë¥¸ ê²ƒì„ ì„ íƒí•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\n"
                    "ë‹µë³€: \"ë‚˜ëŠ” ê·¸ë¥¼ ë³¸ ì ì´ ìˆìŒì„ ê¸°ì–µí•´ ëƒˆë‹¤.\"ê°€ ì˜³ë‹¤. 'ê¸°ì–µí•´ ëƒˆë‹¤'ëŠ” 'ê¸°ì–µí•˜-+-ì•„+ëƒˆë‹¤'ì˜ êµ¬ì„±ì´ë‹¤. ì´ì²˜ëŸ¼ 'ë³¸ìš©ì–¸+-ì•„/-ì–´+ë³´ì¡° ìš©ì–¸' êµ¬ì„±ì¸ ê²½ìš° ë³¸ìš©ì–¸ê³¼ ë³´ì¡° ìš©ì–¸ì„ ë¶™ì—¬ ì“°ëŠ” ê²ƒì´ í—ˆìš©ë˜ì§€ë§Œ, ì´ëŸ¬í•œ êµ¬ì„±ì„ ê°–ë”ë¼ë„ ì•ë§ì´ 3ìŒì ˆ ì´ìƒì˜ í•©ì„±ì–´ë‚˜ íŒŒìƒì–´ë¼ë©´ ë³´ì¡° ìš©ì–¸ì„ ë¶™ì—¬ ì“°ëŠ” ê²ƒì´ í—ˆìš©ë˜ì§€ ì•ŠëŠ”ë‹¤. 'ê¸°ì–µí•˜ë‹¤'ëŠ” 'ê¸°ì–µ'ê³¼ '-í•˜ë‹¤'ê°€ ê²°í•©í•œ íŒŒìƒì–´ì´ë©° 'ê¸°ì–µí•´'ëŠ” 3ìŒì ˆì´ë‹¤. ë”°ë¼ì„œ 'ê¸°ì–µí•´'ì™€ 'ëƒˆë‹¤'ëŠ” ë„ì–´ ì¨ì•¼ í•œë‹¤."
                )
            }

            # question typeì— ë”°ë¥¸ instruction ì„ íƒ
            instruction = type_instructions.get(inp['question_type'], "")

            # RAG ê²€ìƒ‰ ìˆ˜í–‰
            context = ""
            if self.use_rag and self.rag_retriever and self.rag_retriever.is_initialized:
                context = self.rag_retriever.retrieve(inp['question'])

            # ê¸°íƒ€ ì •ë³´ ìƒì„± (questionê³¼ question_type ì œì™¸)
            other_info = {k: v for k, v in inp.items() if k not in ['question', 'question_type']}
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„± ìš”ì†Œë“¤
            chat_parts = [instruction]
            
            # ì°¸ê³ ìë£Œ ì¶”ê°€ (RAG ê²€ìƒ‰ ê²°ê³¼)
            if context:
                chat_parts.append(f"[ì°¸ê³ ìë£Œ]\n{context}")
            
            # ê¸°íƒ€ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
            if other_info:
                info_list = ["[ê¸°íƒ€ ì •ë³´]"]
                for key, value in other_info.items():
                    info_list.append(f"- {key}: {value}")
                chat_parts.append("\n".join(info_list))

            # ì§ˆë¬¸ ì¶”ê°€
            chat_parts.append(f"[ì§ˆë¬¸]\n{inp['question']}")

            # ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„±
            chat = "\n\n".join(chat_parts)

            return chat
        
        print(f"ë°ì´í„° ë¡œë”© ì¤‘... ì´ {len(data)}ê°œ ìƒ˜í”Œ")
        if self.use_rag:
            print("ğŸ” RAG ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
        else:
            print("ğŸ“š ì¼ë°˜ instruction ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        
        for i, example in enumerate(data):
            if i % 50 == 0:  # RAG ê²€ìƒ‰ ë•Œë¬¸ì— ë” ìì£¼ ì¶œë ¥
                print(f"ì²˜ë¦¬ ì¤‘: {i}/{len(data)}")
                
            user_prompt = make_chat(example["input"])
            message = [
                {"role": "system", "content": PROMPT},
                {"role": "user", "content": user_prompt},
            ]
     
            source = tokenizer.apply_chat_template(
                message,
                add_generation_prompt=True,
                return_tensors="pt",
                enable_thinking=False
            )

            target = example.get("output", {}).get("answer", "")
            if target != "":
                target += tokenizer.eos_token
            
            target_tokens = tokenizer(
                target,
                return_attention_mask=False,
                add_special_tokens=False,
                return_tensors="pt"
            )
            target_tokens["input_ids"] = target_tokens["input_ids"].type(torch.int64)

            # ê¸¸ì´ ì œí•œ ì²´í¬ (RAG ì»¨í…ìŠ¤íŠ¸ ë•Œë¬¸ì— ë” ì—„ê²©í•˜ê²Œ)
            total_length = source[0].shape[0] + target_tokens["input_ids"][0].shape[0]
            if total_length > self.max_length:
                # ë„ˆë¬´ ê¸´ ê²½ìš° targetì„ ì¤„ì„
                max_target_length = self.max_length - source[0].shape[0] - 10
                if max_target_length > 0:
                    target_tokens["input_ids"] = target_tokens["input_ids"][:, :max_target_length]
                else:
                    print(f"âš ï¸ ìƒ˜í”Œ {i} ê±´ë„ˆë›°ê¸°: ê¸¸ì´ ì´ˆê³¼ (ì´ ê¸¸ì´: {total_length})")
                    continue  # ê±´ë„ˆë›°ê¸°

            input_ids = torch.concat((source[0], target_tokens["input_ids"][0]))
            labels = torch.concat((
                torch.LongTensor([IGNORE_INDEX] * source[0].shape[0]), 
                target_tokens["input_ids"][0]
            ))
            
            self.inp.append(input_ids)
            self.label.append(labels)
        
        print(f"ë°ì´í„° ë¡œë”© ì™„ë£Œ! ì´ {len(self.inp)}ê°œ ìƒ˜í”Œ ì²˜ë¦¬ë¨")
        if self.use_rag and self.rag_retriever and self.rag_retriever.is_initialized:
            print("âœ… RAG ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ë°ì´í„°ì…‹ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def __len__(self):
        return len(self.inp)

    def __getitem__(self, idx):
        return {
            "input_ids": self.inp[idx],
            "labels": self.label[idx]
        }


class DataCollatorForSupervisedDataset(object):
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def __call__(self, instances):
        input_ids = [instance["input_ids"] for instance in instances]
        labels = [instance["labels"] for instance in instances]
        
        # íŒ¨ë”© ì ìš©
        input_ids = torch.nn.utils.rnn.pad_sequence(
            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id
        )
        labels = torch.nn.utils.rnn.pad_sequence(
            labels, batch_first=True, padding_value=-100
        )
        
        return dict(
            input_ids=input_ids,
            labels=labels,
            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),
        )

import argparse
import json
import time
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path
import requests

# Qdrant ë¡œì»¬ ëª¨ë“œë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue

# ARM64 í˜¸í™˜ì„±ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ
import re
from rank_bm25 import BM25Okapi

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ë¡œì»¬ Qdrant DB ê´€ë ¨ í´ë˜ìŠ¤ë“¤

class SimpleBM25Wrapper:
    """rank_bm25ë¥¼ ì‚¬ìš©í•œ BM25 ë˜í¼ í´ë˜ìŠ¤ (ARM64 í˜¸í™˜)"""
    
    def __init__(self):
        self.bm25 = None
        self.tokenized_docs = []
        self.documents = []
        
    def tokenize(self, text: str) -> List[str]:
        """í•œêµ­ì–´ ì¹œí™”ì  í† í¬ë‚˜ì´ì €"""
        # í•œêµ­ì–´, ì˜ì–´, ìˆ«ìë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì œê±°
        text = re.sub(r'[^\wê°€-í£\s]', ' ', text)
        # ê³µë°±ìœ¼ë¡œ ë¶„í• í•˜ê³  ê¸¸ì´ê°€ 1ë³´ë‹¤ í° í† í°ë§Œ ì„ íƒ
        tokens = [token.lower() for token in text.split() if len(token) > 1]
        return tokens
    
    def fit(self, documents: List[str]):
        """BM25 ëª¨ë¸ í•™ìŠµ"""
        self.documents = documents
        self.tokenized_docs = [self.tokenize(doc) for doc in documents]
        self.bm25 = BM25Okapi(self.tokenized_docs)
        logger.info(f"âœ… BM25 ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
    
    def get_scores(self, query: str) -> List[float]:
        """ì¿¼ë¦¬ì— ëŒ€í•œ BM25 ì ìˆ˜ ê³„ì‚°"""
        if self.bm25 is None:
            logger.error("BM25 ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return [0.0] * len(self.documents)
        
        query_tokens = self.tokenize(query)
        scores = self.bm25.get_scores(query_tokens)
        return scores.tolist()

class OllamaBGEEmbedder:
    """Ollama BGE-M3 ëª¨ë¸ì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì„ë² ë”© í´ë˜ìŠ¤"""
    
    def __init__(self, base_url: str = "http://localhost:11434", model_name: str = "bge-m3"):
        self.base_url = base_url
        self.model_name = model_name
        self.embed_url = f"{base_url}/api/embeddings"
        
    def get_embedding(self, text: str) -> Optional[List[float]]:
        """í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”© ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            payload = {
                "model": self.model_name,
                "prompt": text
            }
            
            response = requests.post(self.embed_url, json=payload, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            embedding = result.get("embedding", [])
            
            if not embedding:
                logger.error(f"ì„ë² ë”© ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {text[:50]}...")
                return None
                
            return embedding
            
        except requests.exceptions.RequestException as e:
            logger.error(f"Ollama API ìš”ì²­ ì‹¤íŒ¨: {e}")
            return None
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

class KoreanRAGVectorDB_Local:
    """ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ ê¸°ë°˜ í•œêµ­ì–´ RAG í•˜ì´ë¸Œë¦¬ë“œ Qdrant ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤"""
    
    def __init__(
        self,
        collection_name: str = "korean_rag_local_collection",
        db_path: str = "./qdrant_storage",
        ollama_host: str = "localhost",
        ollama_port: int = 11434
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            collection_name: Qdrant ì»¬ë ‰ì…˜ ì´ë¦„
            db_path: Qdrant ë¡œì»¬ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê²½ë¡œ
            ollama_host: Ollama ì„œë²„ í˜¸ìŠ¤íŠ¸
            ollama_port: Ollama ì„œë²„ í¬íŠ¸
        """
        self.collection_name = collection_name
        self.db_path = Path(db_path)
        
        # ë¡œì»¬ DB ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.db_path.mkdir(parents=True, exist_ok=True)
        
        # Ollama BGE-M3 ì„ë² ë” ì´ˆê¸°í™”
        ollama_base_url = f"http://{ollama_host}:{ollama_port}"
        self.embedder = OllamaBGEEmbedder(base_url=ollama_base_url)
        
        # ARM64 í˜¸í™˜ BM25 ëª¨ë¸ ì´ˆê¸°í™”
        self.bm25_wrapper = SimpleBM25Wrapper()
        
        # Qdrant ë¡œì»¬ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            self.client = QdrantClient(path=str(self.db_path))
            logger.info(f"âœ… Qdrant ë¡œì»¬ ëª¨ë“œ ì´ˆê¸°í™” ì„±ê³µ: {self.db_path}")
        except Exception as e:
            logger.error(f"âŒ Qdrant ë¡œì»¬ ëª¨ë“œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise ConnectionError(f"Qdrant ë¡œì»¬ ëª¨ë“œë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.db_path}")
        
        # Ollama BGE-M3 ëª¨ë¸ ì—°ê²° í™•ì¸ ë° ë²¡í„° ì°¨ì› í™•ì¸
        self.dense_vector_size = self._check_ollama_model()
        
        # ë¬¸ì„œ ì €ì¥ìš© (BM25 ê³„ì‚°ì„ ìœ„í•´)
        self.documents = []
        self.document_metadata = []
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒì„±
        self._initialize_collection()
    
    def _check_ollama_model(self) -> int:
        """Ollama BGE-M3 ëª¨ë¸ ì—°ê²° í™•ì¸ ë° ë²¡í„° ì°¨ì› ë°˜í™˜"""
        try:
            test_embedding = self.embedder.get_embedding("í…ŒìŠ¤íŠ¸")
            if test_embedding and len(test_embedding) > 0:
                vector_size = len(test_embedding)
                logger.info(f"âœ… Ollama BGE-M3 ëª¨ë¸ ì—°ê²° í™•ì¸. ì„ë² ë”© ì°¨ì›: {vector_size}")
                return vector_size
            else:
                raise Exception("BGE-M3 ëª¨ë¸ì—ì„œ ìœ íš¨í•œ ì„ë² ë”©ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"âŒ Ollama BGE-M3 ëª¨ë¸ ì—°ê²° ì‹¤íŒ¨: {e}")
            logger.error("ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
            logger.error("1. Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€: ollama serve")
            logger.error("2. BGE-M3 ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€: ollama pull bge-m3")
            raise ConnectionError(f"Ollama BGE-M3 ëª¨ë¸ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    
    def _initialize_collection(self):
        """Qdrant ì»¬ë ‰ì…˜ ì´ˆê¸°í™” (ê¸°ì¡´ ì»¬ë ‰ì…˜ í™•ì¸ í›„ ìƒì„± ë˜ëŠ” ë¡œë“œ)"""
        try:
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ í™•ì¸
            collections = self.client.get_collections()
            collection_names = [col.name for col in collections.collections]
            
            if self.collection_name in collection_names:
                # ê¸°ì¡´ ì»¬ë ‰ì…˜ ë¡œë“œ
                logger.info(f"ğŸ“‚ ê¸°ì¡´ ì»¬ë ‰ì…˜ '{self.collection_name}' ë¡œë“œ")
                collection_info = self.client.get_collection(self.collection_name)
                points_count = collection_info.points_count
                logger.info(f"âœ… ê¸°ì¡´ ì»¬ë ‰ì…˜ ë¡œë“œ ì™„ë£Œ: {points_count}ê°œ ë²¡í„°")
                
                # ê¸°ì¡´ ë°ì´í„°ë¡œ BM25 ì´ˆê¸°í™”
                self._load_existing_data_for_bm25()
            else:
                # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
                logger.info(f"ğŸ†• ìƒˆ ì»¬ë ‰ì…˜ '{self.collection_name}' ìƒì„±")
                self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=VectorParams(
                        size=self.dense_vector_size, 
                        distance=Distance.COSINE
                    ),
                    # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
                    hnsw_config={
                        "m": 16,
                        "ef_construct": 200
                    },
                    optimizers_config={
                        "default_segment_number": 2,
                        "max_segment_size": 20000,
                        "memmap_threshold": 20000,
                        "indexing_threshold": 20000,
                        "flush_interval_sec": 5,
                        "max_optimization_threads": 2
                    }
                )
                logger.info(f"âœ… ì»¬ë ‰ì…˜ '{self.collection_name}' ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _load_existing_data_for_bm25(self):
        """ê¸°ì¡´ ë²¡í„°í™”ëœ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ BM25 ì´ˆê¸°í™”"""
        try:
            # ë²¡í„°í™”ëœ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            current_dir = Path(__file__).parent
            vectorized_data_path = current_dir.parent / "02_make_vector_data" / "vectorized_data.json"
            
            if vectorized_data_path.exists():
                with open(vectorized_data_path, 'r', encoding='utf-8') as f:
                    vectorized_data = json.load(f)
                
                # ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì €ì¥ (BM25ìš©)
                documents_text = [item['content'] for item in vectorized_data]
                self.documents = documents_text
                self.document_metadata = vectorized_data
                
                # BM25 ëª¨ë¸ í•™ìŠµ
                if documents_text:
                    self.bm25_wrapper.fit(documents_text)
                    logger.info(f"âœ… BM25 ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: {len(documents_text)}ê°œ ë¬¸ì„œ")
                else:
                    logger.warning("âš ï¸ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                logger.warning("âš ï¸ ë²¡í„°í™”ëœ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. BM25 ê²€ìƒ‰ì´ ì œí•œë©ë‹ˆë‹¤.")
                self.documents = []
                self.document_metadata = []
                
        except Exception as e:
            logger.error(f"âŒ BM25 ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.documents = []
            self.document_metadata = []
    
    def search_hybrid(
        self,
        query: str,
        top_k: int = 5,
        alpha: float = 0.7
    ) -> List[Dict[str, Any]]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰ (Dense + Sparse, HTTP API ì‚¬ìš©)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            alpha: Denseì™€ Sparse ê°€ì¤‘ì¹˜ (0.7 = Dense 70%, Sparse 30%)
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # 1. Dense ê²€ìƒ‰ (Semantic Search, HTTP API)
            query_embedding = self.embedder.get_embedding(query)
            if query_embedding is None:
                logger.error("âŒ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                return []
            
            # ë¡œì»¬ í´ë¼ì´ì–¸íŠ¸ë¥¼ í†µí•´ Dense ê²€ìƒ‰
            dense_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=min(top_k * 2, len(self.documents)) if self.documents else top_k,
                with_payload=True
            )
            
            # 2. BM25 ê²€ìƒ‰ (Sparse, ARM64 í˜¸í™˜)
            bm25_scores = self.bm25_wrapper.get_scores(query) if self.documents else []
            
            # 3. ê²°ê³¼ í†µí•©
            combined_results = {}
            
            # Dense ê²°ê³¼ ì²˜ë¦¬
            for point in dense_results:
                chunk_id = point.payload["chunk_id"]
                combined_results[chunk_id] = {
                    'payload': point.payload,
                    'dense_score': float(point.score),
                    'sparse_score': 0.0
                }
            
            # Sparse ê²°ê³¼ ì²˜ë¦¬
            for i, sparse_score in enumerate(bm25_scores):
                if i < len(self.document_metadata):
                    chunk_id = self.document_metadata[i]['id']
                    if chunk_id in combined_results:
                        combined_results[chunk_id]['sparse_score'] = float(sparse_score)
                    elif sparse_score > 0:  # BM25 ì ìˆ˜ê°€ ìˆëŠ” ë¬¸ì„œë§Œ
                        combined_results[chunk_id] = {
                            'payload': {
                                'chunk_id': chunk_id,
                                'content': self.document_metadata[i]['content'],
                                'original_content': self.document_metadata[i]['original_content'],
                                'length': self.document_metadata[i]['length'],
                                'embedding_dim': self.document_metadata[i].get('embedding_dim', 0)
                            },
                            'dense_score': 0.0,
                            'sparse_score': float(sparse_score)
                        }
            
            # 4. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
            final_results = []
            max_sparse_score = max(bm25_scores) if bm25_scores else 1.0
            
            for chunk_id, result in combined_results.items():
                # ì ìˆ˜ ì •ê·œí™” (0-1 ë²”ìœ„)
                normalized_dense = min(1.0, max(0.0, result['dense_score']))
                normalized_sparse = min(1.0, max(0.0, result['sparse_score'] / max_sparse_score)) if max_sparse_score > 0 else 0.0
                
                # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
                hybrid_score = alpha * normalized_dense + (1 - alpha) * normalized_sparse
                
                payload = result['payload']
                final_result = {
                    'chunk_id': payload['chunk_id'],
                    'content': payload['content'],
                    'original_content': payload['original_content'],
                    'length': payload['length'],
                    'score': hybrid_score,
                    'dense_score': result['dense_score'],
                    'sparse_score': result['sparse_score'],
                    'embedding_dim': payload.get('embedding_dim', 0),
                    'search_type': 'hybrid'
                }
                final_results.append(final_result)
            
            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ ê²°ê³¼ ë°˜í™˜
            final_results.sort(key=lambda x: x['score'], reverse=True)
            return final_results[:top_k]
            
        except Exception as e:
            logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_dense_only(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """Dense ë²¡í„°ë§Œ ì‚¬ìš©í•œ ê²€ìƒ‰ (HTTP API)"""
        try:
            query_embedding = self.embedder.get_embedding(query)
            if query_embedding is None:
                logger.error("âŒ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                return []
            
            search_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=top_k,
                with_payload=True
            )
            
            results = []
            for point in search_results:
                result = {
                    'chunk_id': point.payload['chunk_id'],
                    'content': point.payload['content'],
                    'original_content': point.payload['original_content'],
                    'length': point.payload['length'],
                    'score': point.score,
                    'embedding_dim': point.payload.get('embedding_dim', 0),
                    'search_type': 'dense_only'
                }
                results.append(result)
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ Dense ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

class QuestionPromptGenerator:
    """ì§ˆë¬¸ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°"""
    
    def __init__(self):
        self.type_instructions = {
            "ì„ ë‹¤í˜•": (
                "[ì§ˆë¬¸]ì„ ì˜ ì½ê³  ë‹µë³€ì„ ìƒì„±í•˜ì‹œì˜¤. ë¬¸ì œë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì§€ ë§ˆì‹œì˜¤.\n"
                "[ì§€ì¹¨]\n"
                "ì£¼ì–´ì§„ ë³´ê¸° ì¤‘ì—ì„œ ê°€ì¥ ì ì ˆí•œ ë‹µì„ ìˆ«ìë¡œë§Œ ì‘ë‹µí•˜ì‹œì˜¤.\n\n"
                "[ì˜ˆì‹œ]\n"
                "ì§ˆë¬¸: ë‹¤ìŒ í•œêµ­ì˜ ì „í†µ ë†€ì´ ì¤‘ 'ì¡°ì„ ì‹œëŒ€'ì— í–‰í•œ ë†€ì´ëŠ”?\n"
                "1) ì£¼ì‚¬ìœ„ ë†€ì´\n"
                "2) ê²€ë¬´\n"
                "3) ê²©êµ¬\n"
                "4) ì˜ê³ \n"
                "5) ë¬´ì• ë¬´\n"
                "ë‹µë³€: 3"
            ),
            "ì„œìˆ í˜•": (
                "[ì§ˆë¬¸]ì„ ì˜ ì½ê³  ë‹µë³€ì„ ìƒì„±í•˜ì‹œì˜¤. ë¬¸ì œë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì§€ ë§ˆì‹œì˜¤.\n"
                "[ì§€ì¹¨]\n"
                "ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì™„ì„±ëœ ë¬¸ì¥ìœ¼ë¡œ ì„œìˆ í•˜ì‹œì˜¤.\n\n"
                "[ì˜ˆì‹œ]\n"
                "ì§ˆë¬¸: ëŒ€í•œë¯¼êµ­ì˜ í–‰ì •êµ¬ì—­ ì²´ê³„ë¥¼ ì„œìˆ í•˜ì„¸ìš”.\n"
                "ë‹µë³€: ëŒ€í•œë¯¼êµ­ì˜ í–‰ì •êµ¬ì—­ì€ ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ì§€ì—­ ë‹¨ìœ„ë¡œ ë‚˜ë‰˜ì–´ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ë¨¼ì € íŠ¹ë³„ì‹œì™€ ê´‘ì—­ì‹œë¶€í„° ì‚´í´ë³¼ ìˆ˜ ìˆë‹¤. íŠ¹ë³„ì‹œë¡œëŠ” ìˆ˜ë„ì¸ ì„œìš¸íŠ¹ë³„ì‹œê°€ ìˆìœ¼ë©°, ê´‘ì—­ì‹œì—ëŠ” ì¸ì²œê´‘ì—­ì‹œ, ë¶€ì‚°ê´‘ì—­ì‹œ, ëŒ€ì „ê´‘ì—­ì‹œ, ê´‘ì£¼ê´‘ì—­ì‹œ, ëŒ€êµ¬ê´‘ì—­ì‹œ, ìš¸ì‚°ê´‘ì—­ì‹œ ë“±ì´ í¬í•¨ëœë‹¤. ì´ ì™¸ì—ë„ ëŒ€í•œë¯¼êµ­ì€ ì¼ë°˜ ë„ ë‹¨ìœ„ë¡œ 6ê°œì˜ ë„ë¥¼ ë‘ê³  ìˆëŠ”ë°, ê·¸ ì´ë¦„ì€ ê²½ê¸°ë„, ì¶©ì²­ë¶ë„, ì¶©ì²­ë‚¨ë„, ì „ë¼ë‚¨ë„, ê²½ìƒë¶ë„, ê²½ìƒë‚¨ë„ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. íŠ¹ë³„í•œ ìì¹˜ê¶Œì„ ë¶€ì—¬ë°›ì€ ë„ì¸ íŠ¹ë³„ìì¹˜ë„ë¡œëŠ” ì œì£¼íŠ¹ë³„ìì¹˜ë„, ì „ë¶íŠ¹ë³„ìì¹˜ë„, ê°•ì›íŠ¹ë³„ìì¹˜ë„ê°€ ìˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ íŠ¹ë³„ìì¹˜ì‹œë¡œëŠ” ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œê°€ ì¡´ì¬í•œë‹¤."
            ),
            "ë‹¨ë‹µí˜•": (
                "[ì§ˆë¬¸]ì„ ì˜ ì½ê³  ë‹µë³€ì„ ìƒì„±í•˜ì‹œì˜¤. ë¬¸ì œë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì§€ ë§ˆì‹œì˜¤.\n"
                "[ì§€ì¹¨]\n"
                "ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ 2ë‹¨ì–´ ì´ë‚´ë¡œ ê°„ë‹¨íˆ ë‹µí•˜ì‹œì˜¤.\n\n"
                "[ì˜ˆì‹œ]\n"
                "ì§ˆë¬¸: ì¡°ì„  í›„ê¸°ì˜ ì‹¤í•™ ì‚¬ìƒê°€ë¡œ ëª©ë¯¼ì‹¬ì„œë¥¼ ì“´ ì¸ë¬¼ì€?\n"
                "ë‹µë³€: ì •ì•½ìš©"
            ),
            "êµì •í˜•": (
                "[ì§ˆë¬¸]ì„ ì˜ ì½ê³  ë‹µë³€ì„ ìƒì„±í•˜ì‹œì˜¤. ë¬¸ì œë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì§€ ë§ˆì‹œì˜¤.\n"
                "[ì§€ì¹¨]\n"
                "ì£¼ì–´ì§„ ë¬¸ì¥ì´ ì˜¬ë°”ë¥¸ì§€ íŒë‹¨í•˜ê³ , í‹€ë¦° ê²½ìš° ì˜¬ë°”ë¥´ê²Œ êµì •í•˜ì—¬ \"~ê°€ ì˜³ë‹¤.\" í˜•íƒœë¡œ ë‹µë³€í•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì‹œì˜¤.\n\n"
                "[ì˜ˆì‹œ]\n"
                "ì§ˆë¬¸: ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ì–´ë¬¸ ê·œë²”ì— ë¶€í•©í•˜ì§€ ì•ŠëŠ” ë¶€ë¶„ì„ ì°¾ì•„ ê³ ì¹˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\n\"ì˜¤ëŠ˜ì€ í¼ì¦ ë§ˆì¶”ê¸°ë¥¼ í•´ ë³¼ ê±°ì˜ˆìš”.\"\n"
                "ë‹µë³€: \"ì˜¤ëŠ˜ì€ í¼ì¦ ë§ì¶”ê¸°ë¥¼ í•´ ë³¼ ê±°ì˜ˆìš”.\"ê°€ ì˜³ë‹¤. 'ì œìë¦¬ì— ë§ê²Œ ë¶™ì´ë‹¤, ì£¼ë¬¸í•˜ë‹¤, ë˜‘ë°”ë¥´ê²Œ í•˜ë‹¤, ë¹„êµí•˜ë‹¤' ë“±ì˜ ëœ»ì´ ìˆëŠ” ë§ì€ 'ë§ˆì¶”ë‹¤'ê°€ ì•„ë‹Œ 'ë§ì¶”ë‹¤'ë¡œ ì ëŠ”ë‹¤."
            ),
            "ì„ íƒí˜•": (
                "[ì§ˆë¬¸]ì„ ì˜ ì½ê³  ë‹µë³€ì„ ìƒì„±í•˜ì‹œì˜¤. ë¬¸ì œë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì§€ ë§ˆì‹œì˜¤.\n"
                "[ì§€ì¹¨]\n"
                "ì£¼ì–´ì§„ ë³´ê¸°ë“¤ ì¤‘ì—ì„œ ê°€ì¥ ì ì ˆí•œ ê²ƒì„ ì„ íƒí•˜ì—¬ \"~ê°€ ì˜³ë‹¤.\" í˜•íƒœë¡œ ë‹µë³€í•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì‹œì˜¤.\n\n"
                "[ì˜ˆì‹œ]\n"
                "ì§ˆë¬¸: \"ë‚˜ëŠ” ê·¸ë¥¼ ë³¸ ì ì´ ìˆìŒì„ {ê¸°ì–µí•´ëƒˆë‹¤/ê¸°ì–µí•´ ëƒˆë‹¤}.\" ê°€ìš´ë° ì˜¬ë°”ë¥¸ ê²ƒì„ ì„ íƒí•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\n"
                "ë‹µë³€: \"ë‚˜ëŠ” ê·¸ë¥¼ ë³¸ ì ì´ ìˆìŒì„ ê¸°ì–µí•´ ëƒˆë‹¤.\"ê°€ ì˜³ë‹¤. 'ê¸°ì–µí•´ ëƒˆë‹¤'ëŠ” 'ê¸°ì–µí•˜-+-ì•„+ëƒˆë‹¤'ì˜ êµ¬ì„±ì´ë‹¤. ì´ì²˜ëŸ¼ 'ë³¸ìš©ì–¸+-ì•„/-ì–´+ë³´ì¡° ìš©ì–¸' êµ¬ì„±ì¸ ê²½ìš° ë³¸ìš©ì–¸ê³¼ ë³´ì¡° ìš©ì–¸ì„ ë¶™ì—¬ ì“°ëŠ” ê²ƒì´ í—ˆìš©ë˜ì§€ë§Œ, ì´ëŸ¬í•œ êµ¬ì„±ì„ ê°–ë”ë¼ë„ ì•ë§ì´ 3ìŒì ˆ ì´ìƒì˜ í•©ì„±ì–´ë‚˜ íŒŒìƒì–´ë¼ë©´ ë³´ì¡° ìš©ì–¸ì„ ë¶™ì—¬ ì“°ëŠ” ê²ƒì´ í—ˆìš©ë˜ì§€ ì•ŠëŠ”ë‹¤. 'ê¸°ì–µí•˜ë‹¤'ëŠ” 'ê¸°ì–µ'ê³¼ '-í•˜ë‹¤'ê°€ ê²°í•©í•œ íŒŒìƒì–´ì´ë©° 'ê¸°ì–µí•´'ëŠ” 3ìŒì ˆì´ë‹¤. ë”°ë¼ì„œ 'ê¸°ì–µí•´'ì™€ 'ëƒˆë‹¤'ëŠ” ë„ì–´ ì¨ì•¼ í•œë‹¤."
            )
        }
    
    def generate_prompt(self, question_data: Dict[str, Any]) -> str:
        """ì§ˆë¬¸ ë°ì´í„°ë¡œë¶€í„° í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        # ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸
        logger.info(f"[DEBUG] í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘: {question_data}")
        
        # question_typeì— ë”°ë¥¸ instruction ì„ íƒ
        question_type = question_data.get('question_type', '')
        instruction = self.type_instructions.get(question_type, "")
        
        # ë””ë²„ê¹…: question_typeê³¼ instruction í™•ì¸
        logger.info(f"[DEBUG] question_type: '{question_type}', instruction ì¡´ì¬: {bool(instruction)}")
        
        # ê¸°íƒ€ ì •ë³´ ìƒì„± (questionê³¼ question_type ì œì™¸)
        other_info = {k: v for k, v in question_data.items() if k not in ['question', 'question_type']}
        
        # ê¸°íƒ€ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
        chat_parts = [instruction]
        if other_info:
            info_list = ["[ê¸°íƒ€ ì •ë³´]"]
            for key, value in other_info.items():
                info_list.append(f"- {key}: {value}")
            chat_parts.append("\n".join(info_list))

        # ì§ˆë¬¸ ì¶”ê°€
        question = question_data.get('question', '')
        chat_parts.append(f"[ì§ˆë¬¸]\n{question}")

        # ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„±
        chat = "\n\n".join(chat_parts)
        
        logger.info(f"[DEBUG] ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(chat)}")
        
        return chat

class OllamaQwenChat:
    """Ollama qwen3:14b ëª¨ë¸ì„ ì‚¬ìš©í•œ ì±„íŒ… í´ë˜ìŠ¤"""
    
    def __init__(self, base_url: str = "http://localhost:11434", model_name: str = "qwen3:14b"):
        self.base_url = base_url
        self.model_name = model_name
        self.chat_url = f"{base_url}/api/chat"
        self.generate_url = f"{base_url}/api/generate"
        
    def chat_with_context(
        self, 
        query: str, 
        context: str, 
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> Optional[str]:
        """ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì±„íŒ… ì‘ë‹µ ìƒì„±"""
        try:
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
            if system_prompt is None:
                system_prompt = """# Instructions:
ë‹¹ì‹ ì€ í•œêµ­ì–´ ì–¸ì–´í•™ ë° ë¬¸ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ Context ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
Contextì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³ , ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."""

            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            messages = [
                {
                    "role": "system",
                    "content": system_prompt
                },
                {
                    "role": "user", 
                    "content": f"""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

# Context:
{context}

# Question: {query}

"""
                }
            ]
            
            payload = {
                "model": self.model_name,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "num_predict": max_tokens
                }
            }
            
            logger.info(f"Qwen3 ëª¨ë¸ì—ê²Œ ì§ˆë¬¸: {query[:50]}...")
            start_time = time.time()
            
            response = requests.post(self.chat_url, json=payload, timeout=600)
            response.raise_for_status()
            
            result = response.json()
            answer = result.get("message", {}).get("content", "")
            
            end_time = time.time()
            logger.info(f"âœ… Qwen3 ì‘ë‹µ ìƒì„± ì™„ë£Œ (ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ)")
            
            return answer.strip() if answer else None
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ Ollama API ìš”ì²­ ì‹¤íŒ¨: {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ ì±„íŒ… ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def simple_generate(self, prompt: str, temperature: float = 0.7) -> Optional[str]:
        """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "num_predict": 1000
                }
            }
            
            response = requests.post(self.generate_url, json=payload, timeout=300)
            response.raise_for_status()
            
            result = response.json()
            return result.get("response", "").strip()
            
        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

class KoreanRAGSystem:
    """í•œêµ­ì–´ RAG ì‹œìŠ¤í…œ (ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ ê¸°ë°˜)"""
    
    def __init__(
        self,
        collection_name: str = "korean_rag_local_collection",
        db_path: str = "./qdrant_storage",
        ollama_host: str = "localhost",
        ollama_port: int = 11434,
        llm_model: str = "qwen3:14b"
    ):
        """
        RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            collection_name: Qdrant ì»¬ë ‰ì…˜ ì´ë¦„
            db_path: Qdrant ë¡œì»¬ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê²½ë¡œ
            ollama_host: Ollama ì„œë²„ í˜¸ìŠ¤íŠ¸
            ollama_port: Ollama ì„œë²„ í¬íŠ¸
            llm_model: ì‚¬ìš©í•  LLM ëª¨ë¸ëª…
        """
        # ë²¡í„° DB ì´ˆê¸°í™”
        logger.info("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
        self.vector_db = KoreanRAGVectorDB_Local(
            collection_name=collection_name,
            db_path=db_path,
            ollama_host=ollama_host,
            ollama_port=ollama_port
        )
        
        # Qwen3 ì±„íŒ… ëª¨ë¸ ì´ˆê¸°í™”
        logger.info(f"Ollama {llm_model} ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
        self.chat_model = OllamaQwenChat(
            base_url=f"http://{ollama_host}:{ollama_port}",
            model_name=llm_model
        )
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸° ì´ˆê¸°í™”
        self.prompt_generator = QuestionPromptGenerator()
        
        # ëª¨ë¸ ì—°ê²° í…ŒìŠ¤íŠ¸
        self._test_model_connection()
        
        logger.info("âœ… í•œêµ­ì–´ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def _test_model_connection(self):
        """ëª¨ë¸ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            test_response = self.chat_model.simple_generate("ì•ˆë…•í•˜ì„¸ìš”. ê°„ë‹¨íˆ ì¸ì‚¬í•´ì£¼ì„¸ìš”.")
            if test_response:
                logger.info(f"âœ… Qwen3 ëª¨ë¸ ì—°ê²° í™•ì¸: {test_response[:50]}...")
            else:
                raise Exception("ëª¨ë¸ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"âŒ Qwen3 ëª¨ë¸ ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
    
    def retrieve_context(
        self, 
        query: str, 
        top_k: int = 5, 
        search_type: str = "hybrid",
        alpha: float = 0.7
    ) -> List[Dict[str, Any]]:
        """ì§ˆë¬¸ì— ê´€ë ¨ëœ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
        try:
            logger.info(f"ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘: '{query}' (ë°©ì‹: {search_type}, top_k: {top_k})")
            
            if search_type == "hybrid":
                results = self.vector_db.search_hybrid(query, top_k=top_k, alpha=alpha)
            elif search_type == "dense":
                results = self.vector_db.search_dense_only(query, top_k=top_k)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²€ìƒ‰ ë°©ì‹: {search_type}")
            
            logger.info(f"âœ… {len(results)}ê°œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def format_context(self, search_results: List[Dict[str, Any]]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        if not search_results:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        for i, result in enumerate(search_results, 1):
            content = result.get('content', '')
            context_parts.append(f"<Content>\n{content}\n</Content>")
        
        return "\n\n".join(context_parts)
    
    def answer_question(
        self,
        question: str,
        top_k: int = 5,
        search_type: str = "hybrid",
        alpha: float = 0.7,
        temperature: float = 0.7,
        system_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """RAG ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€"""
        try:
            start_time = time.time()
            
            # 1. ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            logger.info(f"ğŸ” ì§ˆë¬¸: {question}")
            search_results = self.retrieve_context(
                query=question, 
                top_k=top_k, 
                search_type=search_type,
                alpha=alpha
            )
            
            if not search_results:
                return {
                    'question': question,
                    'answer': "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ì„œ ë‹µë³€ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    'context': [],
                    'search_type': search_type,
                    'processing_time': time.time() - start_time
                }
            
            # 2. ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
            context_text = self.format_context(search_results)
            
            context_text = "<Context>" + context_text + "</Context>"
            
            # 3. LLM ë‹µë³€ ìƒì„±
            answer = self.chat_model.chat_with_context(
                query=question,
                context=context_text,
                system_prompt=system_prompt,
                temperature=temperature
            )
            
            if not answer:
                answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            
            end_time = time.time()
            
            result = {
                'question': question,
                'answer': answer,
                'context': search_results,
                'search_type': search_type,
                'search_params': {
                    'top_k': top_k,
                    'alpha': alpha if search_type == "hybrid" else None,
                    'temperature': temperature
                },
                'processing_time': end_time - start_time
            }
            
            logger.info(f"âœ… RAG ë‹µë³€ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {result['processing_time']:.2f}ì´ˆ)")
            return result
            
        except Exception as e:
            logger.error(f"âŒ RAG ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                'question': question,
                'answer': f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                'context': [],
                'search_type': search_type,
                'processing_time': time.time() - start_time,
                'error': str(e)
            }
    
    def process_question_data(self, question_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì§ˆë¬¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ RAG ë‹µë³€ ìƒì„±"""
        try:
            logger.info(f"[DEBUG] ì²˜ë¦¬í•  ì§ˆë¬¸ ë°ì´í„°: {question_data}")
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.prompt_generator.generate_prompt(question_data)
            question = question_data.get('question', '')
            
            logger.info(f"[DEBUG] ìƒì„±ëœ í”„ë¡¬í”„íŠ¸:\n{prompt}")
            logger.info(f"[DEBUG] ì¶”ì¶œëœ ì§ˆë¬¸: {question}")
            
            # RAG ë‹µë³€ ìƒì„±
            result = self.answer_question(
                question=question,
                top_k=5,
                search_type="hybrid",
                alpha=0.7,
                temperature=0.7,
                system_prompt=prompt
            )
            
            # ê²°ê³¼ì— ì›ë³¸ ë°ì´í„° ì •ë³´ ì¶”ê°€
            result['original_data'] = question_data
            result['generated_prompt'] = prompt
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ì§ˆë¬¸ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return {
                'question': question_data.get('question', ''),
                'answer': f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                'context': [],
                'error': str(e),
                'original_data': question_data
            }
    
    def batch_test(self, test_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        results = []
        total_start_time = time.time()
        
        logger.info(f"ğŸš€ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹œì‘: {len(test_data)}ê°œ ì§ˆë¬¸")
        
        for i, raw_data in enumerate(test_data, 1):
            logger.info(f"\n--- í…ŒìŠ¤íŠ¸ {i}/{len(test_data)} ---")
            
            # ë°ì´í„° êµ¬ì¡° ë³€í™˜: {"id": "1", "input": {...}} -> {...}
            if 'input' in raw_data:
                question_data = raw_data['input'].copy()
                question_data['id'] = raw_data.get('id', str(i))
            else:
                question_data = raw_data
            
            result = self.process_question_data(question_data)
            results.append(result)
            
            # ì¤‘ê°„ ê²°ê³¼ ì¶œë ¥
            print(f"\n{'='*60}")
            print(f"ì§ˆë¬¸ {i}: {result['question']}")
            print(f"ë‹µë³€: {result['answer']}")
            print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(result.get('context', []))}")
            print(f"ì²˜ë¦¬ ì‹œê°„: {result.get('processing_time', 0):.2f}ì´ˆ")
            
            if result.get('context'):
                print(f"\nê´€ë ¨ ë¬¸ì„œ (ìƒìœ„ 3ê°œ):")
                for j, ctx in enumerate(result['context'][:3], 1):
                    score = ctx.get('score', 0)
                    content = ctx.get('content', '')[:100]
                    print(f"  {j}. [ì ìˆ˜: {score:.3f}] {content}...")
        
        total_time = time.time() - total_start_time
        logger.info(f"\nâœ… ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ (ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ)")
        
        return results
    
    def interactive_chat(self):
        """ëŒ€í™”í˜• RAG ì±„íŒ…"""
        print("\n" + "="*60)
        print("ğŸ¤– í•œêµ­ì–´ RAG ì‹œìŠ¤í…œ ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸")
        print("- 'quit' ë˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤")
        print("- 'help'ë¥¼ ì…ë ¥í•˜ë©´ ë„ì›€ë§ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        print("="*60)
        
        while True:
            try:
                question = input("\nâ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                
                if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                    print("ğŸ‘‹ RAG ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                if question.lower() == 'help':
                    print("""
ğŸ“– ë„ì›€ë§:
- í•œêµ­ì–´ ì–¸ì–´í•™, ë¬¸ë²•, í‘œì¤€ì–´ ê·œì • ë“±ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”
- ì˜ˆì‹œ ì§ˆë¬¸ë“¤:
  * "í‘œì¤€ì–´ ê·œì •ì´ ë­ì•¼?"
  * "í•œêµ­ì–´ ë¬¸ë²•ì—ì„œ ì¡°ì‚¬ëŠ” ì–´ë–»ê²Œ ì‚¬ìš©í•´?"
  * "ë§ì¶¤ë²• ê·œì¹™ì„ ì•Œë ¤ì¤˜"
  * "ì™¸ë˜ì–´ í‘œê¸°ë²•ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
                    """)
                    continue
                
                if not question:
                    print("âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                # RAG ë‹µë³€ ìƒì„±
                result = self.answer_question(
                    question=question,
                    top_k=5,
                    search_type="hybrid",
                    alpha=0.7,
                    temperature=0.7
                )
                
                # ê²°ê³¼ ì¶œë ¥
                print(f"\nğŸ¤– ë‹µë³€:")
                print(result['answer'])
                
                print(f"\nğŸ“Š ê²€ìƒ‰ ì •ë³´:")
                print(f"- ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(result.get('context', []))}ê°œ")
                print(f"- ê²€ìƒ‰ ë°©ì‹: {result['search_type']}")
                print(f"- ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
                
                # ê´€ë ¨ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°
                if result.get('context'):
                    print(f"\nğŸ“ ê´€ë ¨ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°:")
                    for i, ctx in enumerate(result['context'][:2], 1):
                        score = ctx.get('score', 0)
                        content = ctx.get('content', '')[:150]
                        print(f"  {i}. [ê´€ë ¨ë„: {score:.3f}] {content}...")
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤. RAG ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue

# fmt: off
parser = argparse.ArgumentParser(prog="rag_test", description="í•œêµ­ì–´ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")

g = parser.add_argument_group("Common Parameter")
g.add_argument("--input", type=str, help="ì…ë ¥ JSON íŒŒì¼ ê²½ë¡œ")
g.add_argument("--output", type=str, help="ì¶œë ¥ JSON íŒŒì¼ ê²½ë¡œ")
g.add_argument("--collection_name", type=str, default="korean_rag_local_collection", help="Qdrant ì»¬ë ‰ì…˜ ì´ë¦„")
g.add_argument("--db_path", type=str, default="./qdrant_storage", help="Qdrant ë¡œì»¬ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê²½ë¡œ")
g.add_argument("--ollama_host", type=str, default="localhost", help="Ollama ì„œë²„ í˜¸ìŠ¤íŠ¸")
g.add_argument("--ollama_port", type=int, default=11434, help="Ollama ì„œë²„ í¬íŠ¸")
g.add_argument("--llm_model", type=str, default="qwen3:14b", help="ì‚¬ìš©í•  LLM ëª¨ë¸")
g.add_argument("--mode", type=str, choices=["batch", "interactive"], default="interactive", help="ì‹¤í–‰ ëª¨ë“œ")
# fmt: on

def main(args):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("=== í•œêµ­ì–´ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===")
    
    try:
        # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        rag_system = KoreanRAGSystem(
            collection_name=args.collection_name,
            db_path=args.db_path,
            ollama_host=args.ollama_host,
            ollama_port=args.ollama_port,
            llm_model=args.llm_model
        )
        
        if args.mode == "batch" and args.input:
            # ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
            logger.info(f"ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {args.input}")
            
            # ì…ë ¥ ë°ì´í„° ë¡œë“œ
            with open(args.input, 'r', encoding='utf-8') as f:
                test_data = json.load(f)
            
            # ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            results = rag_system.batch_test(test_data)
            
            # ê²°ê³¼ ì €ì¥
            if args.output:
                import re
                import os
                
                # ë””ë²„ê·¸ íŒŒì¼ëª… ìƒì„±
                base_name = os.path.splitext(args.output)[0]
                ext = os.path.splitext(args.output)[1]
                debug_file = f"{base_name}_debug{ext}"
                
                # 1. ì›ë³¸ ê²°ê³¼ë¥¼ ë””ë²„ê·¸ íŒŒì¼ë¡œ ì €ì¥ (ì „ì²´ ìƒì„¸ ì •ë³´)
                with open(debug_file, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                logger.info(f"ë””ë²„ê·¸ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {debug_file}")
                
                # 2. ì •ì œëœ ê²°ê³¼ë¥¼ ë©”ì¸ íŒŒì¼ë¡œ ì €ì¥
                def clean_answer(answer_text):
                    """answerì—ì„œ <think>...</think> ë¶€ë¶„ì„ ì œê±°í•˜ê³  ì¢Œìš° ì—¬ë°± ì œê±°"""
                    if not answer_text:
                        return ""
                    
                    # <think>...</think> íŒ¨í„´ ì œê±° (ê°œí–‰ ë¬¸ì í¬í•¨)
                    cleaned = re.sub(r'<think>.*?</think>', '', answer_text, flags=re.DOTALL)
                    
                    # ì¢Œìš° ì—¬ë°± ì œê±°
                    cleaned = cleaned.strip()
                    
                    return cleaned
                
                # ì •ì œëœ ê²°ê³¼ ìƒì„±
                cleaned_results = []
                for result in results:
                    original_data = result.get('original_data', {})
                    
                    cleaned_result = {
                        "id": original_data.get('id', ''),
                        "input": original_data.get('input', original_data),
                        "output": {
                            "answer": clean_answer(result.get('answer', ''))
                        }
                    }
                    
                    # inputì´ ë³„ë„ë¡œ ì—†ëŠ” ê²½ìš° ì›ë³¸ ë°ì´í„° êµ¬ì¡° ìœ ì§€
                    if 'input' not in original_data:
                        cleaned_result["input"] = {
                            k: v for k, v in original_data.items() 
                            if k not in ['id']
                        }
                    
                    cleaned_results.append(cleaned_result)
                
                # ì •ì œëœ ê²°ê³¼ ì €ì¥
                with open(args.output, 'w', encoding='utf-8') as f:
                    json.dump(cleaned_results, f, ensure_ascii=False, indent=2)
                logger.info(f"ì •ì œëœ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {args.output}")
            
            # ê²°ê³¼ ìš”ì•½
            print(f"\n{'='*60}")
            print("ğŸ“Š ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
            print(f"{'='*60}")
            
            total_time = sum(r.get('processing_time', 0) for r in results)
            avg_time = total_time / len(results) if results else 0
            
            print(f"ì´ ì§ˆë¬¸ ìˆ˜: {len(results)}")
            print(f"ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
            print(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time:.2f}ì´ˆ")
            
            # ê° ì§ˆë¬¸ë³„ ìš”ì•½
            for i, result in enumerate(results, 1):
                print(f"\n{i}. {result['question']}")
                print(f"   ë‹µë³€ ê¸¸ì´: {len(result['answer'])}ì")
                print(f"   ê²€ìƒ‰ ë¬¸ì„œ: {len(result.get('context', []))}ê°œ")
                print(f"   ì²˜ë¦¬ ì‹œê°„: {result.get('processing_time', 0):.2f}ì´ˆ")
        
        else:
            # ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ ëª¨ë“œ
            rag_system.interactive_chat()
        
        logger.info("=== RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
        
    except Exception as e:
        logger.error(f"âŒ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise

if __name__ == "__main__":
    exit(main(parser.parse_args()))
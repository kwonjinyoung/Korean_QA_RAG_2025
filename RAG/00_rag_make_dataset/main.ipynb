{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCX 파일 경로: /home/orin/Project/Korean_QA_RAG_2025/RAG/resource/rag_data.docx\n",
      "DOCX 파일을 로드하고 청킹을 시작합니다...\n",
      "총 115개의 청크가 생성되었습니다.\n",
      "청크들을 length 기준으로 내림차순 정렬했습니다.\n",
      "청크들이 chunks_docx.jsonl에 저장되었습니다.\n",
      "\n",
      "첫 번째 청크 미리보기 (가장 긴 청크):\n",
      "길이: 2101 문자\n",
      "내용 (처음 200자):\n",
      "한 가지 의미를 나타내는 형태 몇 가지가 널리 쓰이며 표준어 규정에 맞으면, 그 모두를 표준어로 삼는다.\n",
      "- 복수 표준어: 가는-허리/잔-허리, 가락-엿/가래-엿, 가뭄/가물, 가엾다/가엽다, 감감-무소식/감감-소식, 개수-통/설거지-통, 개숫-물/설거지-물, 갱-엿/검은-엿, -거리다/-대다, 거위-배/횟-배, 것/해, 게을러-빠지다/게을러-터지다, 고깃...\n"
     ]
    }
   ],
   "source": [
    "# DOCX 파일을 읽고 정규식 기준으로 청킹하여 JSONL로 저장하는 코드 (python-docx 사용)\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from docx import Document\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def custom_regex_split(text: str, pattern: str = r\"^\\<.+\\>\") -> List[str]:\n",
    "    \"\"\"\n",
    "    정규식 패턴을 기준으로 텍스트를 청킹합니다.\n",
    "    \n",
    "    Args:\n",
    "        text: 분할할 텍스트\n",
    "        pattern: 청크 시작점을 나타내는 정규식 패턴\n",
    "    \n",
    "    Returns:\n",
    "        청킹된 텍스트 리스트\n",
    "    \"\"\"\n",
    "    # 줄별로 분할\n",
    "    lines = text.split('\\n')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # 정규식 패턴에 매치되는 경우 새 청크 시작\n",
    "        if re.match(pattern, line.strip()):\n",
    "            # 이전 청크가 있으면 저장\n",
    "            if current_chunk:\n",
    "                chunks.append('\\n'.join(current_chunk).strip())\n",
    "            # 새 청크 시작\n",
    "            current_chunk = [line]\n",
    "        else:\n",
    "            current_chunk.append(line)\n",
    "    \n",
    "    # 마지막 청크 추가\n",
    "    if current_chunk:\n",
    "        chunks.append('\\n'.join(current_chunk).strip())\n",
    "    \n",
    "    return [chunk for chunk in chunks if chunk.strip()]  # 빈 청크 제거\n",
    "\n",
    "def load_and_chunk_docx(docx_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    DOCX를 로드하고 정규식 기준으로 청킹합니다.\n",
    "    \n",
    "    Args:\n",
    "        docx_path: DOCX 파일 경로\n",
    "    \n",
    "    Returns:\n",
    "        청킹된 문서들의 리스트\n",
    "    \"\"\"\n",
    "    # 파일 존재 확인\n",
    "    if not os.path.isfile(docx_path):\n",
    "        raise ValueError(f\"파일이 존재하지 않습니다: {docx_path}\")\n",
    "        \n",
    "    # DOCX 로드 (python-docx 사용)\n",
    "    doc = Document(docx_path)\n",
    "    \n",
    "    # 모든 paragraph의 텍스트를 하나로 합침\n",
    "    full_text = \"\"\n",
    "    for paragraph in doc.paragraphs:\n",
    "        full_text += paragraph.text + '\\n'\n",
    "    \n",
    "    # 정규식 기준으로 청킹\n",
    "    chunks = custom_regex_split(full_text)\n",
    "    \n",
    "    # chunks에서 첫줄을 제거 (줄넘김 기준으로 첫줄 제거)\n",
    "    processed_chunks = []\n",
    "    for chunk in chunks:\n",
    "        lines = chunk.split('\\n')\n",
    "        if len(lines) > 1:\n",
    "            # 첫 줄을 제거하고 나머지 줄들을 합침\n",
    "            processed_chunk = '\\n'.join(lines[1:])\n",
    "            processed_chunks.append(processed_chunk)\n",
    "        elif len(lines) == 1:\n",
    "            # 한 줄짜리 청크는 빈 문자열로 처리\n",
    "            processed_chunks.append('')\n",
    "        else:\n",
    "            processed_chunks.append('')\n",
    "    \n",
    "    chunks = processed_chunks\n",
    "    \n",
    "    # 결과 포맷팅\n",
    "    result = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if chunk.strip():  # 빈 청크 제외\n",
    "            result.append({\n",
    "                \"content\": chunk,\n",
    "                \"length\": len(chunk),\n",
    "                \"original_content\": chunk\n",
    "            })\n",
    "    \n",
    "    return result\n",
    "\n",
    "def save_to_jsonl(data: List[Dict[str, Any]], output_path: str):\n",
    "    \"\"\"\n",
    "    데이터를 JSONL 형태로 저장합니다.\n",
    "    \n",
    "    Args:\n",
    "        data: 저장할 데이터 리스트\n",
    "        output_path: 출력 파일 경로\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 현재 디렉토리 확인\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    # 절대 경로로 DOCX 파일 경로 설정\n",
    "    docx_path = os.path.abspath(os.path.join(current_dir, \"..\", \"resource\", \"rag_data.docx\"))\n",
    "    output_path = \"chunks_docx.jsonl\"\n",
    "    \n",
    "    # 파일 존재 확인\n",
    "    if not os.path.exists(docx_path):\n",
    "        print(f\"파일을 찾을 수 없습니다: {docx_path}\")\n",
    "        # 대안 경로들 시도\n",
    "        alternative_paths = [\n",
    "            os.path.join(current_dir, \"resource\", \"rag_data.docx\"),\n",
    "            \"/resource/국어 지식 기반 생성(RAG) 참조 문서.docx\"\n",
    "        ]\n",
    "        \n",
    "        for alt_path in alternative_paths:\n",
    "            if os.path.exists(alt_path):\n",
    "                docx_path = alt_path\n",
    "                print(f\"파일을 찾았습니다: {docx_path}\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"모든 경로에서 파일을 찾을 수 없습니다.\")\n",
    "            print(f\"현재 디렉토리: {current_dir}\")\n",
    "            exit(1)\n",
    "    \n",
    "    print(f\"DOCX 파일 경로: {docx_path}\")\n",
    "    print(\"DOCX 파일을 로드하고 청킹을 시작합니다...\")\n",
    "    \n",
    "    # DOCX 로드 및 청킹\n",
    "    chunks = load_and_chunk_docx(docx_path)\n",
    "    \n",
    "    print(f\"총 {len(chunks)}개의 청크가 생성되었습니다.\")\n",
    "    \n",
    "    # length 기준으로 내림차순 정렬\n",
    "    chunks.sort(key=lambda x: x['length'], reverse=True)\n",
    "    print(\"청크들을 length 기준으로 내림차순 정렬했습니다.\")\n",
    "    \n",
    "    # JSONL로 저장\n",
    "    save_to_jsonl(chunks, output_path)\n",
    "    \n",
    "    print(f\"청크들이 {output_path}에 저장되었습니다.\")\n",
    "    \n",
    "    # 첫 번째 청크 미리보기 (가장 긴 청크)\n",
    "    if chunks:\n",
    "        print(f\"\\n첫 번째 청크 미리보기 (가장 긴 청크):\")\n",
    "        print(f\"길이: {chunks[0]['length']} 문자\")\n",
    "        print(f\"내용 (처음 200자):\")\n",
    "        print(chunks[0]['content'][:200] + \"...\" if len(chunks[0]['content']) > 200 else chunks[0]['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 청크에서 908개의 새로운 청크를 생성했습니다.\n",
      "결과가 rechunked_data.jsonl에 저장되었습니다.\n",
      "\n",
      "=== 처리 결과 예시 ===\n",
      "\n",
      "청크 1:\n",
      "길이: 2101\n",
      "내용:\n",
      "한 가지 의미를 나타내는 형태 몇 가지가 널리 쓰이며 표준어 규정에 맞으면, 그 모두를 표준어로 삼는다.\n",
      "- 복수 표준어: 가는-허리/잔-허리, 가락-엿/가래-엿, 가뭄/가물, 가엾다/가엽다, 감감-무소식/감감-소식, 개수-통/설거지-통, 개숫-물/설거지-물, 갱-엿/검은-엿, -거리다/-대다, 거위-배/횟-배, 것/해, 게을러-빠지다/게을러-터지다, 고깃...\n",
      "--------------------------------------------------\n",
      "\n",
      "청크 2:\n",
      "길이: 57\n",
      "내용:\n",
      "(1) 같은 자격의 어구를 열거할 때 그 사이에 쓴다.\n",
      "- 근면, 검소, 협동은 우리 겨레의 미덕이다.\n",
      "--------------------------------------------------\n",
      "\n",
      "청크 3:\n",
      "길이: 74\n",
      "내용:\n",
      "(1) 같은 자격의 어구를 열거할 때 그 사이에 쓴다.\n",
      "- 충청도의 계룡산, 전라도의 내장산, 강원도의 설악산은 모두 국립 공원이다.\n",
      "--------------------------------------------------\n",
      "\n",
      "청크 4:\n",
      "길이: 91\n",
      "내용:\n",
      "(1) 같은 자격의 어구를 열거할 때 그 사이에 쓴다.\n",
      "- 집을 보러 가면 그 집이 내가 원하는 조건에 맞는지, 살기에 편한지, 망가진 곳은 없는지 확인해야 한다.\n",
      "--------------------------------------------------\n",
      "\n",
      "청크 5:\n",
      "길이: 77\n",
      "내용:\n",
      "(1) 같은 자격의 어구를 열거할 때 그 사이에 쓴다.\n",
      "다만, (가) 쉼표 없이도 열거되는 사항임이 쉽게 드러날 때는 쓰지 않을 수 있다.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def process_chunks(input_file, output_file):\n",
    "    \"\"\"\n",
    "    chunks.jsonl의 각 라인을 읽어서 content를 재구성합니다.\n",
    "    각 content의 첫 줄을 헤더로 하고, 나머지 줄들과 조합하여 새로운 청크들을 만듭니다.\n",
    "    \"\"\"\n",
    "    new_chunks = []\n",
    "    \n",
    "    # 파일 경로 확인\n",
    "    current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"파일을 찾을 수 없습니다: {input_file}\")\n",
    "        # 상대 경로 시도\n",
    "        alternative_paths = [\n",
    "            \"chunks_docx.jsonl\",\n",
    "            os.path.join(current_dir, \"chunks_docx.jsonl\"),\n",
    "            os.path.join(current_dir, \"..\", \"00_rag_make_dataset\", \"chunks_docx.jsonl\")\n",
    "        ]\n",
    "        \n",
    "        for alt_path in alternative_paths:\n",
    "            if os.path.exists(alt_path):\n",
    "                input_file = alt_path\n",
    "                print(f\"파일을 찾았습니다: {input_file}\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"모든 경로에서 파일을 찾을 수 없습니다.\")\n",
    "            print(f\"현재 디렉토리: {current_dir}\")\n",
    "            return\n",
    "    \n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip():  # 빈 줄 무시\n",
    "                    item = json.loads(line.strip())\n",
    "                    content = item['content']\n",
    "                    original_content = item['original_content']\n",
    "                    \n",
    "                    # 줄바꿈으로 분할\n",
    "                    lines = content.split('\\n')\n",
    "                    \n",
    "                    if len(lines) >= 2:  # 최소 2줄 이상이어야 처리\n",
    "                        header = lines[0]  # 첫 줄을 헤더로\n",
    "                        \n",
    "                        # 나머지 각 줄과 헤더를 조합\n",
    "                        for i, line_content in enumerate(lines[1:], 1):\n",
    "                            if line_content.strip():  # 빈 줄 제외\n",
    "                                new_content = header + '\\n' + line_content\n",
    "                                new_chunk = {\n",
    "                                    'content': new_content,\n",
    "                                    'length': len(new_content),\n",
    "                                    'original_content': original_content\n",
    "                                }\n",
    "                                new_chunks.append(new_chunk)\n",
    "        \n",
    "        # 새로운 JSONL 파일로 저장\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for chunk in new_chunks:\n",
    "                f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        print(f\"원본 청크에서 {len(new_chunks)}개의 새로운 청크를 생성했습니다.\")\n",
    "        print(f\"결과가 {output_file}에 저장되었습니다.\")\n",
    "        \n",
    "        # 몇 개 예시 출력\n",
    "        print(\"\\n=== 처리 결과 예시 ===\")\n",
    "        for i, chunk in enumerate(new_chunks[:5]):  # 처음 5개만 출력\n",
    "            print(f\"\\n청크 {i+1}:\")\n",
    "            print(f\"길이: {chunk['length']}\")\n",
    "            print(f\"내용:\\n{chunk['content'][:200]}{'...' if len(chunk['content']) > 200 else ''}\")\n",
    "            print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "\n",
    "# 주피터 노트북에서 실행할 때는 상대 경로 사용\n",
    "input_file = \"chunks_docx.jsonl\"\n",
    "output_file = \"rechunked_data.jsonl\"\n",
    "\n",
    "process_chunks(input_file, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

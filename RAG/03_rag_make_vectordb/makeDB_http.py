import json
import os
from typing import List, Dict, Any, Optional
from pathlib import Path
import numpy as np
from tqdm import tqdm
import requests
import time
import logging
import re

# ARM64 í˜¸í™˜ì„±ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ (HTTP API ì‚¬ìš©)
from rank_bm25 import BM25Okapi

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class QdrantHTTPClient:
    """Qdrant HTTP API í´ë¼ì´ì–¸íŠ¸ (ARM64 í˜¸í™˜)"""
    
    def __init__(self, host: str = "localhost", port: int = 6333):
        self.base_url = f"http://{host}:{port}"
        self.session = requests.Session()
        
    def _make_request(self, method: str, endpoint: str, data: Dict = None) -> Dict:
        """HTTP ìš”ì²­ ì‹¤í–‰"""
        url = f"{self.base_url}{endpoint}"
        try:
            if method.upper() == "GET":
                response = self.session.get(url, timeout=60)
            elif method.upper() == "POST":
                response = self.session.post(url, json=data, timeout=60)
            elif method.upper() == "PUT":
                response = self.session.put(url, json=data, timeout=60)
            elif method.upper() == "DELETE":
                response = self.session.delete(url, timeout=60)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” HTTP ë©”ì„œë“œ: {method}")
                
            response.raise_for_status()
            return response.json()
            
        except requests.exceptions.RequestException as e:
            logger.error(f"Qdrant HTTP API ìš”ì²­ ì‹¤íŒ¨: {e}")
            raise
    
    def get_collections(self) -> Dict:
        """ì»¬ë ‰ì…˜ ëª©ë¡ ì¡°íšŒ"""
        return self._make_request("GET", "/collections")
    
    def create_collection(self, collection_name: str, vector_size: int) -> Dict:
        """ì»¬ë ‰ì…˜ ìƒì„±"""
        config = {
            "vectors": {
                "size": vector_size,
                "distance": "Cosine"
            },
            "hnsw_config": {
                "m": 16,
                "ef_construct": 200
            },
            "optimizers_config": {
                "default_segment_number": 2,
                "max_segment_size": 20000,
                "memmap_threshold": 20000,
                "indexing_threshold": 20000,
                "flush_interval_sec": 5,
                "max_optimization_threads": 2
            }
        }
        return self._make_request("PUT", f"/collections/{collection_name}", config)
    
    def delete_collection(self, collection_name: str) -> Dict:
        """ì»¬ë ‰ì…˜ ì‚­ì œ"""
        return self._make_request("DELETE", f"/collections/{collection_name}")
    
    def get_collection_info(self, collection_name: str) -> Dict:
        """ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ"""
        return self._make_request("GET", f"/collections/{collection_name}")
    
    def upsert_points(self, collection_name: str, points: List[Dict]) -> Dict:
        """í¬ì¸íŠ¸ ì—…ì„œíŠ¸"""
        data = {"points": points}
        return self._make_request("PUT", f"/collections/{collection_name}/points", data)
    
    def search_points(self, collection_name: str, query_vector: List[float], 
                     limit: int = 10, with_payload: bool = True) -> Dict:
        """í¬ì¸íŠ¸ ê²€ìƒ‰"""
        data = {
            "vector": query_vector,
            "limit": limit,
            "with_payload": with_payload
        }
        return self._make_request("POST", f"/collections/{collection_name}/points/search", data)

class SimpleBM25Wrapper:
    """rank_bm25ë¥¼ ì‚¬ìš©í•œ BM25 ë˜í¼ í´ë˜ìŠ¤ (ARM64 í˜¸í™˜)"""
    
    def __init__(self):
        self.bm25 = None
        self.tokenized_docs = []
        self.documents = []
        
    def tokenize(self, text: str) -> List[str]:
        """í•œêµ­ì–´ ì¹œí™”ì  í† í¬ë‚˜ì´ì €"""
        # í•œêµ­ì–´, ì˜ì–´, ìˆ«ìë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì œê±°
        text = re.sub(r'[^\wê°€-í£\s]', ' ', text)
        # ê³µë°±ìœ¼ë¡œ ë¶„í• í•˜ê³  ê¸¸ì´ê°€ 1ë³´ë‹¤ í° í† í°ë§Œ ì„ íƒ
        tokens = [token.lower() for token in text.split() if len(token) > 1]
        return tokens
    
    def fit(self, documents: List[str]):
        """BM25 ëª¨ë¸ í•™ìŠµ"""
        self.documents = documents
        self.tokenized_docs = [self.tokenize(doc) for doc in documents]
        self.bm25 = BM25Okapi(self.tokenized_docs)
        logger.info(f"âœ… BM25 ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
    
    def get_scores(self, query: str) -> List[float]:
        """ì¿¼ë¦¬ì— ëŒ€í•œ BM25 ì ìˆ˜ ê³„ì‚°"""
        if self.bm25 is None:
            logger.error("BM25 ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return [0.0] * len(self.documents)
        
        query_tokens = self.tokenize(query)
        scores = self.bm25.get_scores(query_tokens)
        return scores.tolist()

class OllamaBGEEmbedder:
    """Ollama BGE-M3 ëª¨ë¸ì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì„ë² ë”© í´ë˜ìŠ¤"""
    
    def __init__(self, base_url: str = "http://localhost:11434", model_name: str = "bge-m3"):
        self.base_url = base_url
        self.model_name = model_name
        self.embed_url = f"{base_url}/api/embeddings"
        
    def get_embedding(self, text: str) -> Optional[List[float]]:
        """í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”© ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            payload = {
                "model": self.model_name,
                "prompt": text
            }
            
            response = requests.post(self.embed_url, json=payload, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            embedding = result.get("embedding", [])
            
            if not embedding:
                logger.error(f"ì„ë² ë”© ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {text[:50]}...")
                return None
                
            return embedding
            
        except requests.exceptions.RequestException as e:
            logger.error(f"Ollama API ìš”ì²­ ì‹¤íŒ¨: {e}")
            return None
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

class KoreanRAGVectorDB_HTTP:
    """HTTP APIë¥¼ ì‚¬ìš©í•œ ARM64 í˜¸í™˜ í•œêµ­ì–´ RAG í•˜ì´ë¸Œë¦¬ë“œ Qdrant ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤"""
    
    def __init__(
        self,
        collection_name: str = "korean_rag_http_collection",
        qdrant_host: str = "localhost",
        qdrant_port: int = 6333,
        ollama_host: str = "localhost",
        ollama_port: int = 11434
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            collection_name: Qdrant ì»¬ë ‰ì…˜ ì´ë¦„
            qdrant_host: Qdrant ì„œë²„ í˜¸ìŠ¤íŠ¸
            qdrant_port: Qdrant ì„œë²„ í¬íŠ¸
            ollama_host: Ollama ì„œë²„ í˜¸ìŠ¤íŠ¸
            ollama_port: Ollama ì„œë²„ í¬íŠ¸
        """
        self.collection_name = collection_name
        
        # Ollama BGE-M3 ì„ë² ë” ì´ˆê¸°í™”
        ollama_base_url = f"http://{ollama_host}:{ollama_port}"
        self.embedder = OllamaBGEEmbedder(base_url=ollama_base_url)
        
        # ARM64 í˜¸í™˜ BM25 ëª¨ë¸ ì´ˆê¸°í™”
        self.bm25_wrapper = SimpleBM25Wrapper()
        
        # Qdrant HTTP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            self.client = QdrantHTTPClient(host=qdrant_host, port=qdrant_port)
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            collections = self.client.get_collections()
            logger.info(f"âœ… Qdrant HTTP API ì—°ê²° ì„±ê³µ: {qdrant_host}:{qdrant_port}")
        except Exception as e:
            logger.error(f"âŒ Qdrant HTTP API ì—°ê²° ì‹¤íŒ¨: {e}")
            raise ConnectionError(f"Qdrant HTTP APIì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {qdrant_host}:{qdrant_port}")
        
        # Ollama BGE-M3 ëª¨ë¸ ì—°ê²° í™•ì¸ ë° ë²¡í„° ì°¨ì› í™•ì¸
        self.dense_vector_size = self._check_ollama_model()
        
        # ë¬¸ì„œ ì €ì¥ìš© (BM25 ê³„ì‚°ì„ ìœ„í•´)
        self.documents = []
        self.document_metadata = []
        
        # ì»¬ë ‰ì…˜ ìƒì„±
        self._create_collection()
    
    def _check_ollama_model(self) -> int:
        """Ollama BGE-M3 ëª¨ë¸ ì—°ê²° í™•ì¸ ë° ë²¡í„° ì°¨ì› ë°˜í™˜"""
        try:
            test_embedding = self.embedder.get_embedding("í…ŒìŠ¤íŠ¸")
            if test_embedding and len(test_embedding) > 0:
                vector_size = len(test_embedding)
                logger.info(f"âœ… Ollama BGE-M3 ëª¨ë¸ ì—°ê²° í™•ì¸. ì„ë² ë”© ì°¨ì›: {vector_size}")
                return vector_size
            else:
                raise Exception("BGE-M3 ëª¨ë¸ì—ì„œ ìœ íš¨í•œ ì„ë² ë”©ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"âŒ Ollama BGE-M3 ëª¨ë¸ ì—°ê²° ì‹¤íŒ¨: {e}")
            logger.error("ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
            logger.error("1. Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€: ollama serve")
            logger.error("2. BGE-M3 ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€: ollama pull bge-m3")
            raise ConnectionError(f"Ollama BGE-M3 ëª¨ë¸ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    
    def _create_collection(self):
        """Qdrant ì»¬ë ‰ì…˜ ìƒì„± (HTTP API ì‚¬ìš©)"""
        try:
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ í™•ì¸ ë° ì‚­ì œ
            collections_response = self.client.get_collections()
            collection_names = [col["name"] for col in collections_response["result"]["collections"]]
            
            if self.collection_name in collection_names:
                logger.warning(f"âš ï¸ ì»¬ë ‰ì…˜ '{self.collection_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ì‚­ì œ í›„ ì¬ìƒì„±í•©ë‹ˆë‹¤.")
                self.client.delete_collection(self.collection_name)
            
            # Dense ë²¡í„° ì»¬ë ‰ì…˜ ìƒì„±
            response = self.client.create_collection(self.collection_name, self.dense_vector_size)
            
            if response["status"] == "ok":
                logger.info(f"âœ… ì»¬ë ‰ì…˜ '{self.collection_name}' ìƒì„± ì™„ë£Œ")
            else:
                raise Exception(f"ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {response}")
            
        except Exception as e:
            logger.error(f"âŒ ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def load_vectorized_data(self, json_path: str) -> List[Dict[str, Any]]:
        """ë²¡í„°í™”ëœ JSON íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            logger.info(f"âœ… {len(data)}ê°œ ë²¡í„°í™”ëœ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {json_path}")
            return data
            
        except FileNotFoundError:
            logger.error(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
            raise
        except Exception as e:
            logger.error(f"âŒ ë²¡í„°í™”ëœ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def build_vector_database_from_json(self, json_path: str, batch_size: int = 100):
        """
        ë²¡í„°í™”ëœ JSON íŒŒì¼ë¡œë¶€í„° í•˜ì´ë¸Œë¦¬ë“œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• (HTTP API ì‚¬ìš©)
        
        Args:
            json_path: vectorized_data.json íŒŒì¼ ê²½ë¡œ
            batch_size: ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°
        """
        logger.info("ğŸš€ HTTP API ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì‹œì‘...")
        
        # ë²¡í„°í™”ëœ ë°ì´í„° ë¡œë“œ
        vectorized_data = self.load_vectorized_data(json_path)
        
        if not vectorized_data:
            logger.error("âŒ ë¡œë“œëœ ë²¡í„°í™” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì €ì¥ (BM25ìš©)
        documents_text = [item['content'] for item in vectorized_data]
        self.documents = documents_text
        self.document_metadata = vectorized_data
        
        # ARM64 í˜¸í™˜ BM25 ëª¨ë¸ í•™ìŠµ
        logger.info("ARM64 í˜¸í™˜ BM25 ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤...")
        self.bm25_wrapper.fit(documents_text)
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        total_batches = (len(vectorized_data) + batch_size - 1) // batch_size
        
        for batch_idx in tqdm(range(total_batches), desc="ë²¡í„° DB ì—…ë¡œë“œ"):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(vectorized_data))
            batch_data = vectorized_data[start_idx:end_idx]
            
            try:
                # HTTP APIìš© í¬ì¸íŠ¸ ìƒì„± (ê¸°ì¡´ ì„ë² ë”© ì‚¬ìš©)
                points = []
                for item in batch_data:
                    point = {
                        "id": item['id'],
                        "vector": item['embedding'],  # ê¸°ì¡´ì— ìƒì„±ëœ ì„ë² ë”© ì‚¬ìš©
                        "payload": {
                            "chunk_id": item['id'],
                            "content": item['content'],
                            "length": item['length'],
                            "original_content": item['original_content'],
                            "embedding_dim": item.get('embedding_dim', len(item['embedding']))
                        }
                    }
                    points.append(point)
                
                # HTTP APIë¥¼ í†µí•´ Qdrantì— ì—…ë¡œë“œ
                response = self.client.upsert_points(self.collection_name, points)
                
                if response["status"] != "ok":
                    logger.error(f"âŒ ë°°ì¹˜ {batch_idx + 1} ì—…ë¡œë“œ ì‹¤íŒ¨: {response}")
                
            except Exception as e:
                logger.error(f"âŒ ë°°ì¹˜ {batch_idx + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        # ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥
        collection_info = self.client.get_collection_info(self.collection_name)
        points_count = collection_info["result"]["points_count"]
        
        logger.info(f"âœ… HTTP API ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
        logger.info(f"   - ì´ ë²¡í„° ìˆ˜: {points_count}")
        logger.info(f"   - ì»¬ë ‰ì…˜ ì´ë¦„: {self.collection_name}")
        logger.info(f"   - Dense ë²¡í„° ì°¨ì›: {self.dense_vector_size}")
        logger.info(f"   - í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ: BGE-M3 + rank_bm25 (HTTP API)")
    
    def search_hybrid(
        self,
        query: str,
        top_k: int = 5,
        alpha: float = 0.7
    ) -> List[Dict[str, Any]]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰ (Dense + Sparse, HTTP API ì‚¬ìš©)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            alpha: Denseì™€ Sparse ê°€ì¤‘ì¹˜ (0.7 = Dense 70%, Sparse 30%)
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # 1. Dense ê²€ìƒ‰ (Semantic Search, HTTP API)
            query_embedding = self.embedder.get_embedding(query)
            if query_embedding is None:
                logger.error("âŒ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                return []
            
            # HTTP APIë¥¼ í†µí•´ Dense ê²€ìƒ‰
            search_response = self.client.search_points(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=min(top_k * 2, len(self.documents)),
                with_payload=True
            )
            
            dense_results = search_response["result"]
            
            # 2. BM25 ê²€ìƒ‰ (Sparse, ARM64 í˜¸í™˜)
            bm25_scores = self.bm25_wrapper.get_scores(query)
            
            # 3. ê²°ê³¼ í†µí•©
            combined_results = {}
            
            # Dense ê²°ê³¼ ì²˜ë¦¬
            for point in dense_results:
                chunk_id = point["payload"]["chunk_id"]
                combined_results[chunk_id] = {
                    'payload': point["payload"],
                    'dense_score': float(point["score"]),
                    'sparse_score': 0.0
                }
            
            # Sparse ê²°ê³¼ ì²˜ë¦¬
            for i, sparse_score in enumerate(bm25_scores):
                chunk_id = self.document_metadata[i]['id']
                if chunk_id in combined_results:
                    combined_results[chunk_id]['sparse_score'] = float(sparse_score)
                elif sparse_score > 0:  # BM25 ì ìˆ˜ê°€ ìˆëŠ” ë¬¸ì„œë§Œ
                    combined_results[chunk_id] = {
                        'payload': {
                            'chunk_id': chunk_id,
                            'content': self.document_metadata[i]['content'],
                            'original_content': self.document_metadata[i]['original_content'],
                            'length': self.document_metadata[i]['length'],
                            'embedding_dim': self.document_metadata[i].get('embedding_dim', 0)
                        },
                        'dense_score': 0.0,
                        'sparse_score': float(sparse_score)
                    }
            
            # 4. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
            final_results = []
            max_sparse_score = max(bm25_scores) if bm25_scores else 1.0
            
            for chunk_id, result in combined_results.items():
                # ì ìˆ˜ ì •ê·œí™” (0-1 ë²”ìœ„)
                normalized_dense = min(1.0, max(0.0, result['dense_score']))
                normalized_sparse = min(1.0, max(0.0, result['sparse_score'] / max_sparse_score)) if max_sparse_score > 0 else 0.0
                
                # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
                hybrid_score = alpha * normalized_dense + (1 - alpha) * normalized_sparse
                
                payload = result['payload']
                final_result = {
                    'chunk_id': payload['chunk_id'],
                    'content': payload['content'],
                    'original_content': payload['original_content'],
                    'length': payload['length'],
                    'score': hybrid_score,
                    'dense_score': result['dense_score'],
                    'sparse_score': result['sparse_score'],
                    'embedding_dim': payload['embedding_dim'],
                    'search_type': 'hybrid_http_api'
                }
                final_results.append(final_result)
            
            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ ê²°ê³¼ ë°˜í™˜
            final_results.sort(key=lambda x: x['score'], reverse=True)
            return final_results[:top_k]
            
        except Exception as e:
            logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_dense_only(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """Dense ë²¡í„°ë§Œ ì‚¬ìš©í•œ ê²€ìƒ‰ (HTTP API)"""
        try:
            query_embedding = self.embedder.get_embedding(query)
            if query_embedding is None:
                logger.error("âŒ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                return []
            
            search_response = self.client.search_points(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=top_k,
                with_payload=True
            )
            
            results = []
            for point in search_response["result"]:
                result = {
                    'chunk_id': point["payload"]['chunk_id'],
                    'content': point["payload"]['content'],
                    'original_content': point["payload"]['original_content'],
                    'length': point["payload"]['length'],
                    'score': point["score"],
                    'embedding_dim': point["payload"]['embedding_dim'],
                    'search_type': 'dense_only_http'
                }
                results.append(result)
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ Dense ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_sparse_only(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """BM25ë§Œ ì‚¬ìš©í•œ ê²€ìƒ‰ (ARM64 í˜¸í™˜)"""
        try:
            bm25_scores = self.bm25_wrapper.get_scores(query)
            
            # ì ìˆ˜ì™€ ì¸ë±ìŠ¤ë¥¼ í•¨ê»˜ ì •ë ¬
            scored_docs = [(score, i) for i, score in enumerate(bm25_scores)]
            scored_docs.sort(reverse=True, key=lambda x: x[0])
            
            results = []
            for score, i in scored_docs[:top_k]:
                if score > 0:  # ì ìˆ˜ê°€ ìˆëŠ” ë¬¸ì„œë§Œ
                    item = self.document_metadata[i]
                    result = {
                        'chunk_id': item['id'],
                        'content': item['content'],
                        'original_content': item['original_content'],
                        'length': item['length'],
                        'score': float(score),
                        'embedding_dim': item.get('embedding_dim', 0),
                        'search_type': 'sparse_only_http'
                    }
                    results.append(result)
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ Sparse ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def get_collection_info(self) -> Dict:
        """ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ (HTTP API)"""
        try:
            return self.client.get_collection_info(self.collection_name)
        except Exception as e:
            logger.error(f"âŒ ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ë° Qdrant ì„œë²„ ì •ë³´ ì¡°íšŒ"""
        try:
            # ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì •ë³´
            import platform
            system_info = {
                'architecture': platform.machine(),
                'platform': platform.platform(),
                'processor': platform.processor()
            }
            
            # ì»¬ë ‰ì…˜ ëª©ë¡
            collections_response = self.client.get_collections()
            collection_names = [col["name"] for col in collections_response["result"]["collections"]]
            
            # í˜„ì¬ ì»¬ë ‰ì…˜ ì •ë³´
            collection_info = None
            points_count = 0
            if self.collection_name in collection_names:
                collection_info = self.client.get_collection_info(self.collection_name)
                points_count = collection_info["result"]["points_count"]
            
            return {
                'server_status': 'connected',
                'api_type': 'HTTP API',
                'system_info': system_info,
                'collections': collection_names,
                'current_collection': self.collection_name,
                'current_collection_exists': self.collection_name in collection_names,
                'points_count': points_count,
                'dense_vector_size': self.dense_vector_size,
                'hybrid_mode': True,
                'sparse_embedding': 'rank_bm25 (ARM64 í˜¸í™˜)',
                'dense_embedding': 'BGE-M3',
                'implementation': 'HTTP API (ARM64 Optimized)'
            }
            
        except Exception as e:
            logger.error(f"âŒ ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                'server_status': 'error',
                'error': str(e)
            }
    
    def clear_collection(self):
        """í˜„ì¬ ì»¬ë ‰ì…˜ ì‚­ì œ (HTTP API)"""
        try:
            response = self.client.delete_collection(self.collection_name)
            if response["status"] == "ok":
                logger.info(f"âœ… ì»¬ë ‰ì…˜ '{self.collection_name}' ì‚­ì œ ì™„ë£Œ")
            else:
                logger.error(f"âŒ ì»¬ë ‰ì…˜ ì‚­ì œ ì‹¤íŒ¨: {response}")
        except Exception as e:
            logger.error(f"âŒ ì»¬ë ‰ì…˜ ì‚­ì œ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("=== HTTP API ê¸°ë°˜ ARM64 í˜¸í™˜ í•œêµ­ì–´ RAG í•˜ì´ë¸Œë¦¬ë“œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ===")
    
    # ê²½ë¡œ ì„¤ì •
    current_dir = Path(__file__).parent
    vectorized_data_path = current_dir.parent / "02_make_vector_data" / "vectorized_data.json"
    
    try:
        # HTTP API ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ë²¡í„° DB í´ë˜ìŠ¤ ì´ˆê¸°í™”
        logger.info("HTTP API ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
        vector_db = KoreanRAGVectorDB_HTTP(
            collection_name="korean_rag_http_collection",
            qdrant_host="localhost",  # Docker ì»¨í…Œì´ë„ˆ (host ë„¤íŠ¸ì›Œí¬)
            qdrant_port=6333,         # HTTP í¬íŠ¸
            ollama_host="localhost",  # Ollama ì„œë²„ í˜¸ìŠ¤íŠ¸
            ollama_port=11434         # Ollama ì„œë²„ í¬íŠ¸
        )
        
        # ë²¡í„°í™”ëœ ë°ì´í„°ë¡œë¶€í„° í•˜ì´ë¸Œë¦¬ë“œ ë²¡í„° DB êµ¬ì¶•
        logger.info("ë²¡í„°í™”ëœ ë°ì´í„°ë¡œë¶€í„° í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤...")
        vector_db.build_vector_database_from_json(str(vectorized_data_path))
        
        # ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥
        collection_info = vector_db.get_collection_info()
        if collection_info and collection_info["status"] == "ok":
            points_count = collection_info["result"]["points_count"]
            logger.info(f"ìµœì¢… ì»¬ë ‰ì…˜ ì •ë³´:")
            logger.info(f"  - ë²¡í„° ê°œìˆ˜: {points_count}")
            logger.info(f"  - Dense ë²¡í„° ì°¨ì›: {vector_db.dense_vector_size}")
            logger.info(f"  - í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ: BGE-M3 + rank_bm25 (HTTP API)")
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        logger.info("\n=== HTTP API í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ===")
        test_queries = ["í‘œì¤€ì–´ ê·œì •", "í•œêµ­ì–´ ë¬¸ë²•", "ì–¸ì–´í•™ ì´ë¡ "]
        
        for query in test_queries:
            logger.info(f"\ní…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: '{query}'")
            
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Dense 70%, Sparse 30%)
            hybrid_results = vector_db.search_hybrid(query, top_k=3, alpha=0.7)
            if hybrid_results:
                logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ ({len(hybrid_results)}ê°œ):")
                for i, result in enumerate(hybrid_results, 1):
                    logger.info(f"  {i}. [í•˜ì´ë¸Œë¦¬ë“œ: {result['score']:.4f}, Dense: {result['dense_score']:.4f}, Sparse: {result['sparse_score']:.4f}]")
                    logger.info(f"     {result['content'][:100]}...")
            
            # Dense only ê²€ìƒ‰
            dense_results = vector_db.search_dense_only(query, top_k=3)
            if dense_results:
                logger.info(f"Dense only ê²€ìƒ‰ ê²°ê³¼ ({len(dense_results)}ê°œ):")
                for i, result in enumerate(dense_results, 1):
                    logger.info(f"  {i}. [Dense: {result['score']:.4f}] {result['content'][:100]}...")
            
            # Sparse only ê²€ìƒ‰
            sparse_results = vector_db.search_sparse_only(query, top_k=3)
            if sparse_results:
                logger.info(f"Sparse only ê²€ìƒ‰ ê²°ê³¼ ({len(sparse_results)}ê°œ):")
                for i, result in enumerate(sparse_results, 1):
                    logger.info(f"  {i}. [Sparse: {result['score']:.4f}] {result['content'][:100]}...")
            
        # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
        system_info = vector_db.get_system_info()
        logger.info(f"\n=== ì‹œìŠ¤í…œ ë° ì„œë²„ ì •ë³´ ===")
        logger.info(f"ì•„í‚¤í…ì²˜: {system_info.get('system_info', {}).get('architecture')}")
        logger.info(f"í”Œë«í¼: {system_info.get('system_info', {}).get('platform')}")
        logger.info(f"API íƒ€ì…: {system_info.get('api_type')}")
        logger.info(f"ì„œë²„ ìƒíƒœ: {system_info.get('server_status')}")
        logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ: {system_info.get('hybrid_mode')}")
        logger.info(f"Dense ì„ë² ë”©: {system_info.get('dense_embedding')}")
        logger.info(f"Sparse ì„ë² ë”©: {system_info.get('sparse_embedding')}")
        logger.info(f"êµ¬í˜„ ë°©ì‹: {system_info.get('implementation')}")
            
        logger.info("\n=== HTTP API ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ ===")
        
    except Exception as e:
        logger.error(f"âŒ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise

if __name__ == "__main__":
    main() 
import json
import os
from typing import List, Dict, Any, Optional
from pathlib import Path
import requests
import time
import logging

# HTTP API ê¸°ë°˜ Qdrant í´ë¼ì´ì–¸íŠ¸ ì„í¬íŠ¸
from makeDB_http import KoreanRAGVectorDB_HTTP

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OllamaQwenChat:
    """Ollama Qwen3:8b-fp16 ëª¨ë¸ì„ ì‚¬ìš©í•œ ì±„íŒ… í´ë˜ìŠ¤"""
    
    def __init__(self, base_url: str = "http://localhost:11434", model_name: str = "qwen3:8b-fp16"):
        self.base_url = base_url
        self.model_name = model_name
        self.chat_url = f"{base_url}/api/chat"
        self.generate_url = f"{base_url}/api/generate"
        
    def chat_with_context(
        self, 
        query: str, 
        context: str, 
        system_prompt: str = None,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> Optional[str]:
        """ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì±„íŒ… ì‘ë‹µ ìƒì„±"""
        try:
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
            if system_prompt is None:
                system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ì–¸ì–´í•™ ë° ë¬¸ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³ , ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."""

            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            messages = [
                {
                    "role": "system",
                    "content": system_prompt
                },
                {
                    "role": "user", 
                    "content": f"""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

**ì»¨í…ìŠ¤íŠ¸:**
{context}

**ì§ˆë¬¸:** {query}

**ë‹µë³€:**"""
                }
            ]
            
            payload = {
                "model": self.model_name,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "num_predict": max_tokens
                }
            }
            
            logger.info(f"Qwen3 ëª¨ë¸ì—ê²Œ ì§ˆë¬¸: {query[:50]}...")
            start_time = time.time()
            
            response = requests.post(self.chat_url, json=payload, timeout=600)
            response.raise_for_status()
            
            result = response.json()
            answer = result.get("message", {}).get("content", "")
            
            end_time = time.time()
            logger.info(f"âœ… Qwen3 ì‘ë‹µ ìƒì„± ì™„ë£Œ (ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ)")
            
            return answer.strip() if answer else None
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ Ollama API ìš”ì²­ ì‹¤íŒ¨: {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ ì±„íŒ… ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def simple_generate(self, prompt: str, temperature: float = 0.7) -> Optional[str]:
        """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "num_predict": 1000
                }
            }
            
            response = requests.post(self.generate_url, json=payload, timeout=300)
            response.raise_for_status()
            
            result = response.json()
            return result.get("response", "").strip()
            
        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

class KoreanRAGSystem:
    """í•œêµ­ì–´ RAG ì‹œìŠ¤í…œ (HTTP API ê¸°ë°˜)"""
    
    def __init__(
        self,
        collection_name: str = "korean_rag_http_collection",
        qdrant_host: str = "localhost",
        qdrant_port: int = 6333,
        ollama_host: str = "localhost",
        ollama_port: int = 11434,
        llm_model: str = "qwen3:8b-fp16"
    ):
        """
        RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            collection_name: Qdrant ì»¬ë ‰ì…˜ ì´ë¦„
            qdrant_host: Qdrant ì„œë²„ í˜¸ìŠ¤íŠ¸
            qdrant_port: Qdrant ì„œë²„ í¬íŠ¸
            ollama_host: Ollama ì„œë²„ í˜¸ìŠ¤íŠ¸
            ollama_port: Ollama ì„œë²„ í¬íŠ¸
            llm_model: ì‚¬ìš©í•  LLM ëª¨ë¸ëª…
        """
        # ë²¡í„° DB ì´ˆê¸°í™” (ê¸°ì¡´ ì»¬ë ‰ì…˜ ë¡œë“œ)
        logger.info("ê¸°ì¡´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²°í•©ë‹ˆë‹¤...")
        self.vector_db = self._connect_existing_vectordb(
            collection_name, qdrant_host, qdrant_port, ollama_host, ollama_port
        )
        
        # Qwen3 ì±„íŒ… ëª¨ë¸ ì´ˆê¸°í™”
        logger.info(f"Ollama {llm_model} ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
        self.chat_model = OllamaQwenChat(
            base_url=f"http://{ollama_host}:{ollama_port}",
            model_name=llm_model
        )
        
        # ëª¨ë¸ ì—°ê²° í…ŒìŠ¤íŠ¸
        self._test_model_connection()
        
        logger.info("âœ… í•œêµ­ì–´ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def _connect_existing_vectordb(
        self, collection_name: str, qdrant_host: str, qdrant_port: int,
        ollama_host: str, ollama_port: int
    ) -> KoreanRAGVectorDB_HTTP:
        """ê¸°ì¡´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²° ë° BM25 ë°ì´í„° ë¡œë“œ"""
        try:
            # ê¸°ì¡´ êµ¬í˜„ì„ ìˆ˜ì •í•˜ì—¬ ì»¬ë ‰ì…˜ì„ ìƒˆë¡œ ìƒì„±í•˜ì§€ ì•Šë„ë¡ í•¨
            vector_db = KoreanRAGVectorDB_HTTP.__new__(KoreanRAGVectorDB_HTTP)
            vector_db.collection_name = collection_name
            
            # Ollama BGE-M3 ì„ë² ë” ì´ˆê¸°í™”
            from makeDB_http import OllamaBGEEmbedder, SimpleBM25Wrapper, QdrantHTTPClient
            ollama_base_url = f"http://{ollama_host}:{ollama_port}"
            vector_db.embedder = OllamaBGEEmbedder(base_url=ollama_base_url)
            
            # BM25 ë˜í¼ ì´ˆê¸°í™”
            vector_db.bm25_wrapper = SimpleBM25Wrapper()
            
            # Qdrant HTTP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            vector_db.client = QdrantHTTPClient(host=qdrant_host, port=qdrant_port)
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            collections = vector_db.client.get_collections()
            collection_names = [col["name"] for col in collections["result"]["collections"]]
            
            if collection_name not in collection_names:
                raise Exception(f"ì»¬ë ‰ì…˜ '{collection_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € makeDB_http.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            
            # ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸
            collection_info = vector_db.client.get_collection_info(collection_name)
            points_count = collection_info["result"]["points_count"]
            vector_config = collection_info["result"]["config"]["params"]["vectors"]
            vector_db.dense_vector_size = vector_config["size"]
            
            # ê¸°ì¡´ ë²¡í„°í™”ëœ ë°ì´í„° ë¡œë“œí•˜ì—¬ BM25ìš© ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            logger.info("BM25 ê²€ìƒ‰ì„ ìœ„í•´ ê¸°ì¡´ ë²¡í„°í™”ëœ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
            current_dir = Path(__file__).parent
            vectorized_data_path = current_dir.parent / "02_make_vector_data" / "vectorized_data.json"
            
            if vectorized_data_path.exists():
                with open(vectorized_data_path, 'r', encoding='utf-8') as f:
                    vectorized_data = json.load(f)
                
                # ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì €ì¥ (BM25ìš©)
                documents_text = [item['content'] for item in vectorized_data]
                vector_db.documents = documents_text
                vector_db.document_metadata = vectorized_data
                
                # BM25 ëª¨ë¸ í•™ìŠµ
                vector_db.bm25_wrapper.fit(documents_text)
                logger.info(f"âœ… BM25 ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {len(documents_text)}ê°œ ë¬¸ì„œ")
            else:
                logger.warning("âš ï¸ ë²¡í„°í™”ëœ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. BM25 ê²€ìƒ‰ì´ ì œí•œë©ë‹ˆë‹¤.")
                vector_db.documents = []
                vector_db.document_metadata = []
            
            logger.info(f"âœ… ê¸°ì¡´ ë²¡í„° DB ì—°ê²° ì„±ê³µ: {points_count}ê°œ ë²¡í„°, {vector_db.dense_vector_size}ì°¨ì›")
            return vector_db
            
        except Exception as e:
            logger.error(f"âŒ ê¸°ì¡´ ë²¡í„° DB ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
    
    def _test_model_connection(self):
        """ëª¨ë¸ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            test_response = self.chat_model.simple_generate("ì•ˆë…•í•˜ì„¸ìš”. ê°„ë‹¨íˆ ì¸ì‚¬í•´ì£¼ì„¸ìš”.")
            if test_response:
                logger.info(f"âœ… Qwen3 ëª¨ë¸ ì—°ê²° í™•ì¸: {test_response[:50]}...")
            else:
                raise Exception("ëª¨ë¸ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"âŒ Qwen3 ëª¨ë¸ ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
    
    def retrieve_context(
        self, 
        query: str, 
        top_k: int = 5, 
        search_type: str = "hybrid",
        alpha: float = 0.7
    ) -> List[Dict[str, Any]]:
        """ì§ˆë¬¸ì— ê´€ë ¨ëœ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
        try:
            logger.info(f"ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘: '{query}' (ë°©ì‹: {search_type}, top_k: {top_k})")
            
            if search_type == "hybrid":
                results = self.vector_db.search_hybrid(query, top_k=top_k, alpha=alpha)
            elif search_type == "dense":
                results = self.vector_db.search_dense_only(query, top_k=top_k)
            elif search_type == "sparse":
                results = self.vector_db.search_sparse_only(query, top_k=top_k)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²€ìƒ‰ ë°©ì‹: {search_type}")
            
            logger.info(f"âœ… {len(results)}ê°œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def format_context(self, search_results: List[Dict[str, Any]]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        if not search_results:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        for i, result in enumerate(search_results, 1):
            score = result.get('score', 0)
            content = result.get('content', '')
            context_parts.append(f"[ë¬¸ì„œ {i}] (ê´€ë ¨ë„: {score:.3f})\n{content}")
        
        return "\n\n".join(context_parts)
    
    def answer_question(
        self,
        question: str,
        top_k: int = 5,
        search_type: str = "hybrid",
        alpha: float = 0.7,
        temperature: float = 0.7,
        system_prompt: str = None
    ) -> Dict[str, Any]:
        """RAG ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€"""
        try:
            start_time = time.time()
            
            # 1. ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            logger.info(f"ğŸ” ì§ˆë¬¸: {question}")
            search_results = self.retrieve_context(
                query=question, 
                top_k=top_k, 
                search_type=search_type,
                alpha=alpha
            )
            
            if not search_results:
                return {
                    'question': question,
                    'answer': "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ì„œ ë‹µë³€ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    'context': [],
                    'search_type': search_type,
                    'processing_time': time.time() - start_time
                }
            
            # 2. ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
            context_text = self.format_context(search_results)
            
            # 3. LLM ë‹µë³€ ìƒì„±
            answer = self.chat_model.chat_with_context(
                query=question,
                context=context_text,
                system_prompt=system_prompt,
                temperature=temperature
            )
            
            if not answer:
                answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            
            end_time = time.time()
            
            result = {
                'question': question,
                'answer': answer,
                'context': search_results,
                'search_type': search_type,
                'search_params': {
                    'top_k': top_k,
                    'alpha': alpha if search_type == "hybrid" else None,
                    'temperature': temperature
                },
                'processing_time': end_time - start_time
            }
            
            logger.info(f"âœ… RAG ë‹µë³€ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {result['processing_time']:.2f}ì´ˆ)")
            return result
            
        except Exception as e:
            logger.error(f"âŒ RAG ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                'question': question,
                'answer': f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                'context': [],
                'search_type': search_type,
                'processing_time': time.time() - start_time,
                'error': str(e)
            }
    
    def batch_test(self, test_questions: List[str], **kwargs) -> List[Dict[str, Any]]:
        """ì—¬ëŸ¬ ì§ˆë¬¸ì— ëŒ€í•œ ë°°ì¹˜ í…ŒìŠ¤íŠ¸"""
        results = []
        total_start_time = time.time()
        
        logger.info(f"ğŸš€ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹œì‘: {len(test_questions)}ê°œ ì§ˆë¬¸")
        
        for i, question in enumerate(test_questions, 1):
            logger.info(f"\n--- í…ŒìŠ¤íŠ¸ {i}/{len(test_questions)} ---")
            result = self.answer_question(question, **kwargs)
            results.append(result)
            
            # ì¤‘ê°„ ê²°ê³¼ ì¶œë ¥
            print(f"\n{'='*60}")
            print(f"ì§ˆë¬¸ {i}: {question}")
            print(f"ë‹µë³€: {result['answer']}")
            print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(result.get('context', []))}")
            print(f"ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
            
            if result.get('context'):
                print(f"\nê´€ë ¨ ë¬¸ì„œ (ìƒìœ„ 3ê°œ):")
                for j, ctx in enumerate(result['context'][:3], 1):
                    score = ctx.get('score', 0)
                    content = ctx.get('content', '')[:100]
                    print(f"  {j}. [ì ìˆ˜: {score:.3f}] {content}...")
        
        total_time = time.time() - total_start_time
        logger.info(f"\nâœ… ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ (ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ)")
        
        return results
    
    def interactive_chat(self):
        """ëŒ€í™”í˜• RAG ì±„íŒ…"""
        print("\n" + "="*60)
        print("ğŸ¤– í•œêµ­ì–´ RAG ì‹œìŠ¤í…œ ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸")
        print("- 'quit' ë˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤")
        print("- 'help'ë¥¼ ì…ë ¥í•˜ë©´ ë„ì›€ë§ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        print("="*60)
        
        while True:
            try:
                question = input("\nâ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                
                if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                    print("ğŸ‘‹ RAG ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                if question.lower() == 'help':
                    print("""
ğŸ“– ë„ì›€ë§:
- í•œêµ­ì–´ ì–¸ì–´í•™, ë¬¸ë²•, í‘œì¤€ì–´ ê·œì • ë“±ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”
- ì˜ˆì‹œ ì§ˆë¬¸ë“¤:
  * "í‘œì¤€ì–´ ê·œì •ì´ ë­ì•¼?"
  * "í•œêµ­ì–´ ë¬¸ë²•ì—ì„œ ì¡°ì‚¬ëŠ” ì–´ë–»ê²Œ ì‚¬ìš©í•´?"
  * "ë§ì¶¤ë²• ê·œì¹™ì„ ì•Œë ¤ì¤˜"
  * "ì™¸ë˜ì–´ í‘œê¸°ë²•ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
                    """)
                    continue
                
                if not question:
                    print("âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                # RAG ë‹µë³€ ìƒì„±
                result = self.answer_question(
                    question=question,
                    top_k=5,
                    search_type="hybrid",
                    alpha=0.7,
                    temperature=0.7
                )
                
                # ê²°ê³¼ ì¶œë ¥
                print(f"\nğŸ¤– ë‹µë³€:")
                print(result['answer'])
                
                print(f"\nğŸ“Š ê²€ìƒ‰ ì •ë³´:")
                print(f"- ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(result.get('context', []))}ê°œ")
                print(f"- ê²€ìƒ‰ ë°©ì‹: {result['search_type']}")
                print(f"- ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
                
                # ê´€ë ¨ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°
                if result.get('context'):
                    print(f"\nğŸ“ ê´€ë ¨ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°:")
                    for i, ctx in enumerate(result['context'][:2], 1):
                        score = ctx.get('score', 0)
                        content = ctx.get('content', '')[:150]
                        print(f"  {i}. [ê´€ë ¨ë„: {score:.3f}] {content}...")
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤. RAG ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    logger.info("=== í•œêµ­ì–´ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===")
    
    try:
        # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        rag_system = KoreanRAGSystem(
            collection_name="korean_rag_http_collection",
            qdrant_host="localhost",
            qdrant_port=6333,
            ollama_host="localhost",
            ollama_port=11434,
            llm_model="qwen3:8b-fp16"
        )
        
        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
        test_questions = [
            "í‘œì¤€ì–´ ê·œì •ì´ ë¬´ì—‡ì¸ê°€ìš”?",
            "í•œêµ­ì–´ ë¬¸ë²•ì—ì„œ ì¡°ì‚¬ì˜ ì—­í• ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ì™¸ë˜ì–´ í‘œê¸°ë²•ì˜ ê¸°ë³¸ ì›ì¹™ì„ ì•Œë ¤ì£¼ì„¸ìš”",
            "ë§ì¶¤ë²•ì—ì„œ ë„ì–´ì“°ê¸° ê·œì¹™ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
            "í•œê¸€ ë§ì¶¤ë²•ì˜ ê¸°ë³¸ ì›ë¦¬ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
        ]
        
        print("\n" + "="*60)
        print("ğŸ§ª ë°°ì¹˜ í…ŒìŠ¤íŠ¸ vs ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ ì„ íƒ")
        print("1. ë°°ì¹˜ í…ŒìŠ¤íŠ¸ (ë¯¸ë¦¬ ì •ì˜ëœ ì§ˆë¬¸ë“¤)")
        print("2. ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ (ì§ì ‘ ì§ˆë¬¸ ì…ë ¥)")
        print("="*60)
        
        choice = input("ì„ íƒí•˜ì„¸ìš” (1 ë˜ëŠ” 2): ").strip()
        
        if choice == "1":
            # ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            logger.info("ë°°ì¹˜ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            results = rag_system.batch_test(
                test_questions=test_questions,
                top_k=5,
                search_type="hybrid",
                alpha=0.7,
                temperature=0.7
            )
            
            # ê²°ê³¼ ìš”ì•½
            print(f"\n{'='*60}")
            print("ğŸ“Š ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
            print(f"{'='*60}")
            
            total_time = sum(r['processing_time'] for r in results)
            avg_time = total_time / len(results)
            
            print(f"ì´ ì§ˆë¬¸ ìˆ˜: {len(results)}")
            print(f"ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
            print(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time:.2f}ì´ˆ")
            
            # ê° ì§ˆë¬¸ë³„ ìš”ì•½
            for i, result in enumerate(results, 1):
                print(f"\n{i}. {result['question']}")
                print(f"   ë‹µë³€ ê¸¸ì´: {len(result['answer'])}ì")
                print(f"   ê²€ìƒ‰ ë¬¸ì„œ: {len(result.get('context', []))}ê°œ")
                print(f"   ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
        
        elif choice == "2":
            # ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            rag_system.interactive_chat()
        
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1 ë˜ëŠ” 2ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        logger.info("=== RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
        
    except Exception as e:
        logger.error(f"âŒ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise

if __name__ == "__main__":
    main()